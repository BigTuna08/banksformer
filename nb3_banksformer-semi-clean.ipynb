{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swymtxpl7W7w"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook can train a model to generate sythetic data.   \n",
    "Ensure the 'ds_suffix' matches the one used to generated the dataset (Under \"Set input dataset\" & in create_dataset notebook)  \n",
    "Parameters for generating data (seq_len, number of seqs) are near bottom (Under \"Generate Full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JjJJyJTZYebt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pXzVhU34zWEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cCvXbPkccV1"
   },
   "source": [
    "### Set input dataset and nb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_KEY_ORDER is ['td_sc', 'month', 'day', 'dow', 'tcode_num', 'log_amount_sc']\n",
      "If this is not correct, edit my_lib/field_config.py and re-run notebook\n"
     ]
    }
   ],
   "source": [
    "from my_lib.field_config import *\n",
    "ds_suffix = \"vf10k\"\n",
    "nb_id = \"vf1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39487, 81, 126), (39487, 80, 6), (39487,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tensor = np.load(f\"stored_data/inp_tensor-{ds_suffix}.npy\")\n",
    "tar_tensor = np.load(f\"stored_data/tar_tensor-{ds_suffix}.npy\")\n",
    "attributes = np.load(f\"stored_data/attributes-{ds_suffix}.npy\")\n",
    "\n",
    "inp_tensor.shape, tar_tensor.shape, attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqs, n_steps, n_feat_inp = inp_tensor.shape\n",
    "n_feat_tar = tar_tensor.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import load_data_encoder\n",
    "data_encoder = load_data_encoder(ds_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any fields not here will have activation=None\n",
    "ACTIVATIONS = {\n",
    "    \"td_sc\": \"relu\",\n",
    "    \"log_amount_sc\": \"relu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and create tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_cv, inds_tr, inds_cv, targ_tr, targ_cv = train_test_split(\n",
    "    inp_tensor, np.arange(n_seqs), tar_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((81, 126), (80, 6)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tr = tf.data.Dataset.from_tensor_slices((x_tr.astype(np.float32), targ_tr.astype(np.float32)))\n",
    "ds_cv = tf.data.Dataset.from_tensor_slices((x_cv.astype(np.float32), targ_cv.astype(np.float32)))\n",
    "\n",
    "ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BUN_jLBTwNxk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import make_batches\n",
    "\n",
    "BUFFER_SIZE = ds_tr.cardinality().numpy()\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_batches = make_batches(ds_tr, BUFFER_SIZE, BATCH_SIZE)\n",
    "val_batches = make_batches(ds_cv, BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "sample_batch = next(iter(train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVxS8OPI9uI0",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SoX0-vd1hue",
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZOJUSB1T8GjM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "loss_scce_logit = SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "loss_scce_probit = SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "\n",
    "loss_mse = MeanSquaredError(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(tf.reduce_sum(seq, axis=2), 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_parts = []\n",
    "    loss_parts_weighted = []\n",
    "\n",
    "    for k, k_pred in pred.items():\n",
    "\n",
    "        st = FIELD_STARTS_TAR[k]\n",
    "        end = st + FIELD_DIMS_TAR[k]\n",
    "\n",
    "        if k in ONE_HOT_DIMS:\n",
    "            loss_ = loss_scce_logit(real[:, :, st:end], k_pred)\n",
    "        elif k in CLOCK_FIELDS:\n",
    "            loss_ = loss_scce_probit(real[:, :, st:end], clock_to_onehot(k, k_pred))\n",
    "#         elif k == \"td\":  # just use mse\n",
    "#             loss_ = tf.cast(loss_td(real[:, :, st:end], k_pred), tf.float32)\n",
    "        else:\n",
    "            loss_ = loss_mse(real[:, :, st:end], k_pred)\n",
    "#         print(k, loss_.dtype)\n",
    "\n",
    "        mask = tf.math.logical_not(tf.math.equal(tf.reduce_sum(real, axis=2), 0))\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask) \n",
    "\n",
    "        loss_parts.append(loss_)\n",
    "        loss_parts_weighted.append(loss_ * LOSS_WEIGHTS[k])\n",
    "\n",
    "    return tf.reduce_sum(loss_parts_weighted), loss_parts\n",
    "\n",
    "\n",
    "\n",
    "# def mse_loss(real, pred):\n",
    "#     return tf.squeeze((real - pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import bulk_encode_time_value\n",
    "\n",
    "EPS_CLOCKP = 0.01\n",
    "\n",
    "CLOCKS = {}\n",
    "for k, val in CLOCK_FIELDS.items():\n",
    "    CLOCKS[k] = tf.constant(bulk_encode_time_value(np.arange(val), val), dtype=tf.float32)\n",
    "\n",
    "def clock_to_probs(pt, pts):\n",
    "    \n",
    "    ds = tf.constant(pts) - pt\n",
    "    sq_ds = np.sum(tf.square(ds+EPS_CLOCKP), axis=1)\n",
    "    raw_ps = 1/ sq_ds   \n",
    "    \n",
    "    return raw_ps / np.sum(raw_ps)\n",
    "\n",
    "\n",
    "\n",
    "def clock_to_onehot(k, vals):\n",
    "    orig_shape = vals.shape\n",
    "\n",
    "    vals = tf.reshape(vals, (-1, orig_shape[-1]))\n",
    "\n",
    "    return np.array([clock_to_probs(p, CLOCKS[k]) for p in vals]).reshape(*orig_shape[:-1], -1)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 31, 'dow': 7, 'month': 12}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLOCK_FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lnJn5SLA2ahP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_layers_enc = 4\n",
    "# num_layers_dec = 1\n",
    "# d_model = 128\n",
    "# dff = 512\n",
    "# num_heads = 4\n",
    "# dropout_rate = 0.1\n",
    "\n",
    "\n",
    "num_layers_enc = 2\n",
    "num_layers_dec = 1\n",
    "d_model = 8\n",
    "dff = 32\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "ONE_HOT_DIMS, FIELD_DIMS, FIELD_STARTS, FIELD_DIMS_TAR, FIELD_STARTS_TAR = get_field_info(ds_suffix)\n",
    "\n",
    "config[\"PRE_DATE_ORDER\"] = PRE_DATE_ORDER\n",
    "config[\"DATE_ORDER\"] = DATE_ORDER\n",
    "config[\"POST_DATE_ORDER\"] = POST_DATE_ORDER\n",
    "config[\"FIELD_STARTS\"] = FIELD_STARTS\n",
    "config[\"FIELD_DIMS\"] = FIELD_DIMS\n",
    "config[\"ACTIVATIONS\"] = ACTIVATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbvmaKNiznHZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:18\n",
      "Begin running num_layers_dec_4-d_model_64-num_heads_4-i_0-dr_0.1-dff_64-opt_adam-l_loss_mse_lw2\n",
      "Epoch 1 Batch 0 Loss 15.6123\n",
      "Epoch 1 Batch 50 Loss 9.2563\n",
      "Epoch 1 Batch 100 Loss 8.5927\n",
      "Epoch 1 Batch 150 Loss 8.2661\n",
      "Epoch 1 Batch 200 Loss 8.0056\n",
      "Epoch 1 Batch 250 Loss 7.8216\n",
      "Epoch 1 Batch 300 Loss 7.6767\n",
      "Epoch 1 Batch 350 Loss 7.5673\n",
      "Epoch 1 Batch 400 Loss 7.4741\n",
      "Epoch 1 Batch 450 Loss 7.3966\n",
      "Epoch 1 Loss 7.3461\n",
      "** on validation data loss is 6.6349\n",
      "Not recording acc: 'Transformer' object has no attribute 'acc_function'\n",
      "Time taken for 1 epoch: 2335.86 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/num_layers_dec_4-d_model_64-num_heads_4-i_0-dr_0__1-dff_64-opt_adam-l_loss_mse_lw2-vf10k-vf1/ckpt-1\n",
      "Epoch 2 Batch 0 Loss 6.7894\n",
      "Epoch 2 Batch 50 Loss 6.6960\n",
      "Epoch 2 Batch 100 Loss 6.6537\n",
      "Epoch 2 Batch 150 Loss 6.6496\n",
      "Epoch 2 Batch 200 Loss 6.6298\n",
      "Epoch 2 Batch 250 Loss 6.6205\n",
      "Epoch 2 Batch 300 Loss 6.6204\n",
      "Epoch 2 Batch 350 Loss 6.6243\n",
      "Epoch 2 Batch 400 Loss 6.6212\n",
      "Epoch 2 Batch 450 Loss 6.6158\n",
      "Epoch 2 Loss 6.6134\n",
      "** on validation data loss is 6.6295\n",
      "Time taken for 1 epoch: 2388.97 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/num_layers_dec_4-d_model_64-num_heads_4-i_0-dr_0__1-dff_64-opt_adam-l_loss_mse_lw2-vf10k-vf1/ckpt-2\n",
      "Epoch 3 Batch 0 Loss 6.7614\n",
      "Epoch 3 Batch 50 Loss 6.5345\n",
      "Epoch 3 Batch 100 Loss 6.5545\n"
     ]
    }
   ],
   "source": [
    "from my_lib.BanksformerGen import Transformer\n",
    "\n",
    "EARLY_STOP = 2\n",
    "EPOCHS = 80\n",
    "\n",
    "opt_name = \"adam\"\n",
    "dr = 0.1\n",
    "                \n",
    "dff = 128\n",
    "num_layers_dec = 4\n",
    "d_model = 128\n",
    "\n",
    "\n",
    "all_models = []\n",
    "for_df = []\n",
    "\n",
    "\n",
    "def to_num(x):\n",
    "    try: return int(x)\n",
    "    except: return float(x)\n",
    "\n",
    "    \n",
    "def id_str_to_folder(id_str):\n",
    "    return id_str.replace(\".\", \"__\")\n",
    "beta = 1\n",
    "\n",
    "# \n",
    "# lws = [LOSS_WEIGHTS_0, LOSS_WEIGHTS_1, LOSS_WEIGHTS_2, LOSS_WEIGHTS_3]\n",
    "\n",
    "LOSS_WEIGHTS = {'balance': 0.25,\n",
    " 'td_sc':1.,\n",
    " 'year': 0.5,\n",
    " 'month': 0.15,\n",
    " 'day': 0.25,\n",
    " 'dow': 0.1,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "lws = [LOSS_WEIGHTS]\n",
    "\n",
    "# td_loss_fns = [(poisson_loss, \"poisson_loss\"), (expon_loss, \"expon_loss\"), (loss_mse, \"loss_mse\")]\n",
    "td_loss_fns = [(loss_mse, \"loss_mse\")]\n",
    "\n",
    "for i in range(1):\n",
    "    for dff in [64]:\n",
    "        for td_loss_fn, name in td_loss_fns:\n",
    "            for d_model in [64]:\n",
    "                for num_heads in [4]:\n",
    "                \n",
    "                    loss_td = td_loss_fn\n",
    "\n",
    "                \n",
    "                    print(datetime.datetime.now().strftime(\"%H:%M\"))\n",
    "\n",
    "\n",
    "                    transformer = Transformer(\n",
    "                        num_layers_enc=num_layers_enc, num_layers_dec=num_layers_dec,\n",
    "                        d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        dff=dff,\n",
    "                        maximum_position_encoding=256,\n",
    "                       net_info = FIELD_DIMS.items(), \n",
    "                        inp_dim = n_feat_inp,\n",
    "                        final_dim= max(n_feat_tar, n_feat_inp),\n",
    "                        config=config,\n",
    "                        rate=dr)\n",
    "                    \n",
    "                    optimizer = tf.keras.optimizers.Adam()\n",
    "                    transformer.optimizer =  optimizer\n",
    "                    \n",
    "#                     LOSS_WEIGHTS = lws[lwi]\n",
    "                    transformer.loss_function = loss_function\n",
    "                    transformer.LOSS_WEIGHTS = LOSS_WEIGHTS\n",
    "\n",
    "                    id_str = f\"num_layers_dec_{num_layers_dec}-d_model_{d_model}-num_heads_{num_heads}-i_{i}\\\n",
    "-dr_{dr}-dff_{dff}-opt_{opt_name}-l_{name}_lw2\"\n",
    "                    \n",
    "                    print(\"Begin running\", id_str)\n",
    "                    transformer.id_str = id_str\n",
    "\n",
    "\n",
    "                    all_models.append(transformer)\n",
    "                    transformer.compile()\n",
    "                    \n",
    "                    \n",
    "                    transformer.checkpoint_path = f\"./checkpoints/{id_str_to_folder(transformer.id_str)}-{ds_suffix}-{nb_id}\"\n",
    "                    transformer.ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                                               optimizer=optimizer)\n",
    "                    transformer.ckpt_manager = tf.train.CheckpointManager(transformer.ckpt, \n",
    "                                                                          transformer.checkpoint_path, max_to_keep=EARLY_STOP)\n",
    "                    \n",
    "#                     if transformer.ckpt_manager.latest_checkpoint:\n",
    "#                         transformer.ckpt.restore(transformer.ckpt_manager.latest_checkpoint)\n",
    "#                         print('Latest checkpoint restored!!')    \n",
    "#                         continue\n",
    "              \n",
    "                    transformer.fit(train_batches, x_cv, targ_cv, epochs= EPOCHS, early_stop=EARLY_STOP, print_every=50, ckpt_every = 1)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                    for_df.append((num_layers_dec, d_model, num_heads, i, dr, beta, dff , name,\n",
    "                                   np.min(transformer.results[\"val_loss\"]), opt_name, transformer.id_str))\n",
    "                    \n",
    "                    df = pd.DataFrame.from_records(for_df, columns=['num_layers_dec', 'd_model', 'num_heads', 'i', \"dr\", \"beta\",\\\n",
    "                                                                    \"dff\", \"loss name\",\n",
    "                                                                    \"val loss\", \"opt name\",\"id_str\"]).sort_values(\"val loss\")\n",
    "                    \n",
    "                    df.to_csv(f\"generation_results/df{datetime.datetime.now().strftime('%H_%M')}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cv.shape, targ_cv.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoder.n_tcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(for_df, columns=['num_layers_dec', 'd_model', 'num_heads', 'i', \"dr\", \"beta\", \"dff\", \"lwi\",\n",
    "                                                \"val loss\", \"opt name\",\"id_str\"]).sort_values(\"val loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df.sort_values(\"val loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df.sort_values([\"lwi\", \"val loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = all_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_YEARS_SPAN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_DIMS.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import encode_time_value\n",
    "#, decode_time_value\n",
    "\n",
    "clocks = {}\n",
    "for max_val in [7, 31, 12]:\n",
    "    cmd = f\"clocks[{max_val}] = np.array([encode_time_value(val, {max_val}) for val in range({max_val})])\"\n",
    "    print(\"Running\", cmd)\n",
    "    exec(cmd)\n",
    "    \n",
    "clocks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import factorial\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "#############  Signatures of pdf/pmfs are (pred, real), this is opposite of losses  #############\n",
    "\n",
    "# def pmf_poisson(l, k):\n",
    "#     return l**k * np.exp(-l) / factorial(k)\n",
    "\n",
    "\n",
    "# def expon_pdf(l, x):\n",
    "#     return l * np.exp(-l*x)\n",
    "\n",
    "\n",
    "def norm_pdf(mean, x):\n",
    "    return norm.pdf(x, loc=mean)\n",
    "    \n",
    "pmf = norm_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = data_encoder.START_DATE \n",
    "\n",
    "if type(START_DATE) == str:\n",
    "    START_DATE = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "    \n",
    "    \n",
    "\n",
    "END_DATE = START_DATE.replace(year = START_DATE.year+ MAX_YEARS_SPAN)\n",
    "\n",
    "ALL_DATES = [START_DATE + datetime.timedelta(i) for i in range((END_DATE - START_DATE).days)]\n",
    "\n",
    "AD = np.array([(d.month % 12, d.day % 31, d.weekday() % 7, i, d.year) for i, d in enumerate(ALL_DATES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import create_masks\n",
    "\n",
    "    \n",
    "def reencode_net_prediction(net_name, predictions):\n",
    "    \n",
    "    date_info = {'month':12, 'day':31, 'dow':7}\n",
    "    batch_size = predictions.shape[0]\n",
    "    \n",
    "    if net_name in ['balance', 'td_sc', 'dss', \"log_amount_sc\"]:\n",
    "        return predictions\n",
    "    \n",
    "#     elif net_name == \"year\":\n",
    "#         return tf.round(predictions/YEAR_SCALE)*YEAR_SCALE\n",
    "    \n",
    "    elif net_name in date_info.keys():\n",
    "        return bulk_nearest_clock_enc(predictions, max_val=date_info[net_name])\n",
    "    \n",
    "    elif net_name == \"tcode_num\":\n",
    "        tcode_len = ONE_HOT_DIMS[\"tcode_num\"]\n",
    "        choices = np.arange(tcode_len)\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, data_encoder.n_tcodes)\n",
    "        choosen =  np.reshape([np.random.choice(choices, p=p) for p in ps], newshape=(batch_size, -1))\n",
    "        return tf.one_hot(choosen, depth=tcode_len)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f\"Got invalid net_name: {net_name}\")\n",
    "\n",
    "days_per_month = np.array([(datetime.date(1990, month, 1) - datetime.timedelta(1)).day for month in range(1,13)]) # 0 = dec\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def get_short_name(tcode):\n",
    "    return short_names[tcode]\n",
    "\n",
    "# @np.vectorize\n",
    "# def get_date_str(yyyy, mm, dd):\n",
    "#     return f\"{yyyy}/{mm:02d}/{dd:02d}\"\n",
    "\n",
    "@np.vectorize\n",
    "def get_date_str(mm, dd):\n",
    "    return f\"{mm:02d}/{dd:02d}\"\n",
    "\n",
    "\n",
    "def bulk_decode(seqs, start_dates, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    # *****\n",
    "#     ages = age_scaler.inverse_transform(seqs[:, 0, :])\n",
    "    ages = seqs[:, 0, :] * data_encoder.ATTR_SCALE\n",
    "    seqs = seqs[:, 1:, :]\n",
    "    assert np.sum(np.diff(ages)) == 0, f\"Bad formating, expected all entries same in each row, got {ages}\"\n",
    "\n",
    "    \n",
    "    amts = seqs[:, :, FIELD_STARTS[\"log_amount_sc\"]].numpy() * data_encoder.LOG_AMOUNT_SCALE\n",
    "    amts = 10 ** amts\n",
    "    amts = np.round(amts - 1.0, 2)\n",
    "\n",
    "\n",
    "    days_passed = np.round(seqs[:, :, FIELD_STARTS[\"td_sc\"]] *data_encoder.TD_SCALE ).astype(int)\n",
    "  \n",
    "\n",
    "#     years = np.round(seqs[:, :, FIELD_STARTS[\"year\"]]/ YEAR_SCALE).astype(int) + START_YEAR\n",
    "\n",
    "    months = bulk_nearest_clock_ind(seqs[:, :, FIELD_STARTS[\"month\"]: FIELD_STARTS[\"month\"] +2], 12)\n",
    "    \n",
    "    days = bulk_nearest_clock_ind(seqs[:, :, FIELD_STARTS[\"day\"]: FIELD_STARTS[\"day\"] +2], 31)\n",
    "    days[days==0] = days_per_month[months[days==0]]\n",
    "    months[months==0] = 12 # needs to be done after days (above)\n",
    "    date_fields = get_date_str(months, days)\n",
    "    \n",
    "    dpc = np.cumsum(days_passed, axis=1) \n",
    "    dates = np.array([[start_dates[i] + datetime.timedelta(int(d)) for d in dpc[i]]for i in range(len(start_dates))])\n",
    "    \n",
    "    tcode_inds = np.argmax(seqs[:, :, FIELD_STARTS[\"tcode_num\"]: FIELD_STARTS[\"tcode_num\"] + FIELD_DIMS[\"tcode_num\"]], axis=-1)\n",
    "#     tcodes = get_short_name(tcode_inds)\n",
    "\n",
    "    ages = np.repeat(ages[:, 0:1], amts.shape[1], axis=1).astype(int)\n",
    "    \n",
    "    return_vals = amts, tcode_inds, date_fields, days_passed, ages, dates\n",
    "    return_lbls = \"amount\", \"tcode_nums\", \"date_fields\", \"days_passed\", \"age\", \"date\"\n",
    "\n",
    "#     print(\"Shapes of amts, tcode_inds, dates, days_passed, ages\\n\", \n",
    "#           amts.shape, tcode_inds.shape, dates.shape, days_passed.shape, ages.shape)\n",
    "#     print(\"days_passed\", days_passed, type(days_passed))\n",
    "    \n",
    "    if return_df_list:\n",
    "        return [pd.DataFrame.from_records(zip(*x), columns=return_lbls) for x in zip(*return_vals)]\n",
    "    \n",
    "    if return_single_df:\n",
    "        return pd.DataFrame.from_records([x for x in zip(*[x.reshape(-1) for x in return_vals])], columns=return_lbls)\n",
    "    \n",
    "    return return_vals\n",
    "\n",
    "\n",
    "\n",
    "def nearest_clock_ind(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return np.argmin(d_sq)\n",
    "\n",
    "\n",
    "def nearest_clock_enc(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return clock[np.argmin(d_sq)]\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_ind(encs, max_val):\n",
    "    batch_size = encs.shape[0]\n",
    "    inds =  np.array([nearest_clock_ind(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, 2))])\n",
    "    return inds.reshape((batch_size, -1))\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_enc(encs, max_val):\n",
    "\n",
    "    batch_size = encs.shape[0]\n",
    "    new_encs =  np.array([nearest_clock_enc(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, 2))])\n",
    "    \n",
    "    return new_encs.reshape((batch_size, -1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.repeat(np.array([1,2,3])[:, None, None], repeats=n_feat_inp, axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seqs(length, ages, start_dates, greedy_dates = False, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    if return_single_df and return_df_list:\n",
    "        raise Exception(\"At most one of: 'return_single_df' and 'return_df_list' can be true\")\n",
    "    \n",
    "    date_inds = np.array([(d - START_DATE).days for d in start_dates])\n",
    "    \n",
    "    max_length = length\n",
    "\n",
    "    output = np.repeat(np.array(ages)[:, None, None], repeats=n_feat_inp, axis=2) / data_encoder.ATTR_SCALE\n",
    "    \n",
    "    raw_preds = []\n",
    "    raw_preds.append(output)\n",
    "\n",
    "    date_info = None\n",
    "    \n",
    "    \n",
    "    for i in range(max_length):\n",
    "\n",
    "\n",
    "        combined_mask, dec_padding_mask = create_masks(output)\n",
    "\n",
    "        predictions, attn, raw_ps, date_inds, enc_preds, date_info = call_to_generate(transformer, output, \n",
    "                                                 True, \n",
    "                                                 combined_mask, \n",
    "                                                 dec_padding_mask, date_inds, date_info, greedy_dates =greedy_dates)\n",
    "\n",
    "        \n",
    "        raw_preds.append(raw_ps)\n",
    "\n",
    "        enc_preds = tf.reshape(tf.constant(enc_preds), shape=(-1,1, n_feat_inp))\n",
    "\n",
    "        output = tf.concat([output, enc_preds], axis=1)\n",
    "\n",
    "        \n",
    "    return bulk_decode(output, start_dates, return_single_df, return_df_list), output, raw_preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Forward pass through transformer\n",
    "# \n",
    "# Returns: preds, attn_w, raw_preds, inds\n",
    "# the returned preds have multiple timesteps, but we only \n",
    "# care about the last (it's the only new one)\n",
    "def call_to_generate(transformer, tar, training,\n",
    "           look_ahead_mask, dec_padding_mask, start_inds, prev_date_info=None, greedy_dates = True):\n",
    "    \n",
    "\n",
    "    ### Pass through decoder stack ###\n",
    "    dec_output, attention_weights = transformer.decoder(\n",
    "        tar, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "\n",
    "    final_output = transformer.final_layer(dec_output) \n",
    "\n",
    "    \n",
    "    \n",
    "    ### Predict each field  ###\n",
    "    preds = {}\n",
    "    raw_preds = {}\n",
    "    encoded_preds = []\n",
    "    \n",
    "    \n",
    "    ## Pre date fields \n",
    "    for net_name in transformer.pre_date_order:  \n",
    "        \n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "        pred = reencode_net_prediction(net_name, pred) # keeps time step\n",
    "        preds[net_name] = pred\n",
    "        \n",
    "        \n",
    "        encoded_preds.append(pred[:,-1,:])\n",
    "        final_output = tf.concat([final_output, pred], axis=2)\n",
    "        \n",
    "        \n",
    "    ## Date fields\n",
    "    date_parts = {}\n",
    "    for net_name in transformer.date_fields:  \n",
    "        \n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "    # Combine info from all predicted date fields (day, month, dow, td)\n",
    "    pred_date, inds = raw_dates_to_reencoded(raw_preds, start_inds, greedy_decode =greedy_dates)\n",
    "    preds[\"date\"] = pred_date\n",
    "\n",
    "    \n",
    "    encoded_preds.append(pred_date[:,-1,:])\n",
    "    \n",
    "    \n",
    "    # Note to self -> what does this do?\n",
    "    if not prev_date_info is None:   # For first step may be None, or a starting date\n",
    "        pred_date = tf.concat([prev_date_info, pred_date], axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "    final_output = tf.concat([final_output, pred_date], axis=2)  \n",
    "    \n",
    "          \n",
    "    ## Post date fields\n",
    "    for net_name in transformer.post_date_order:  \n",
    "#         print(net_name)\n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "#         print(pred.shape)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "        pred = reencode_net_prediction(net_name, pred)\n",
    "        preds[net_name] = pred\n",
    "        \n",
    "        encoded_preds.append(pred[:,-1,:])\n",
    "        final_output = tf.concat([final_output, pred], axis=-1)   \n",
    "        \n",
    "    \n",
    "#     print(\"start_inds + inds \\n\", start_inds + inds)\n",
    "#     print(\"\\n\\npred_date\\n\", pred_date)\n",
    "#     print(\"\\n\"*5)\n",
    "        \n",
    "    return preds, attention_weights, raw_preds, start_inds + inds, tf.expand_dims(tf.concat(encoded_preds, axis=1), axis=1), pred_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMF_EPS = 1e-6\n",
    "\n",
    "# Takes raw predictions (info about predicted day, month, dow, and days passed) and start inds \n",
    "# (indicate the current date for each of the seqs) \n",
    "# Computes a number of days passed for each based on inputs (either greedily or with sampling)\n",
    "# returns the new_dates (old_dates + days passed) and their indicies\n",
    "def raw_dates_to_reencoded(raw, start_inds,  max_days = 100, greedy_decode=False):\n",
    "    \n",
    "    all_ps = [clock_to_onehot(k, raw[k][:,-1]) for k in [\"month\", \"day\", \"dow\"]]\n",
    "\n",
    "    timesteps = np.zeros(len(start_inds)).astype(int)\n",
    "\n",
    "    for i, (month_ps, day_ps, dow_ps, l_pred, si) in enumerate(zip(*all_ps, raw[\"td_sc\"][:,-1].numpy(), start_inds)):\n",
    "\n",
    "        ps = month_ps[AD[si:si+max_days,0]]*day_ps[AD[si:si+max_days,1]]*dow_ps[AD[si:si+max_days,2]] * \\\n",
    "                pmf(max(PMF_EPS, l_pred)*data_encoder.TD_SCALE, AD[si:si+max_days,3]-si ) \n",
    "\n",
    "        \n",
    "        if greedy_decode:\n",
    "            timesteps[i] = np.argmax(ps)\n",
    "        else:\n",
    "#             print(\"max_days\", \"len(ps)\" ,max_days, len(ps))\n",
    "            timesteps[i] = np.random.choice(max_days, p=ps/sum(ps))\n",
    "        \n",
    "        \n",
    "    inds = start_inds + timesteps\n",
    "    \n",
    "\n",
    "    return tf.expand_dims(\n",
    "                tf.concat([tf.expand_dims(\n",
    "                           timesteps.astype(np.float32)/ data_encoder.TD_SCALE, axis=1), \n",
    "#                            AD[inds, 4:5]*YEAR_SCALE,\n",
    "                           bulk_encode_time_value(AD[inds, 0], 12),\n",
    "                           bulk_encode_time_value(AD[inds, 1], 31),\n",
    "                           bulk_encode_time_value(AD[inds, 2], 7)\n",
    "              ], axis=1), axis=1), timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD[[0,3], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoder.n_tcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_dfs, seqs, raw = generate_seqs(length= 25, \n",
    "                          ages=[75, 25], \n",
    "                          start_dates=[START_DATE, START_DATE+datetime.timedelta(days=1)], \n",
    "                          greedy_dates=False,\n",
    "                          return_df_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_dfs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 80\n",
    "n_seqs_to_generate = len(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = np.random.choice([START_DATE + datetime.timedelta(i) for i in range(365)], size=n_seqs_to_generate)\n",
    "start_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_ages = np.random.choice(attributes, size=n_seqs_to_generate)\n",
    "seq_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "full_df, seqs, raw = generate_seqs(length= seq_len, \n",
    "                                   ages=seq_ages, \n",
    "                                   start_dates= start_dates, \n",
    "                                   return_single_df=True )\n",
    "\n",
    "print(f\"took {time.time() - start} secs to generate\")\n",
    "\n",
    "save_as = f\"generated_data/gen_{id_str_to_folder(transformer.id_str)}--{nb_id}-len_{seq_len}.csv\"\n",
    "\n",
    "\n",
    "full_df.to_csv(save_as)\n",
    "print(\"Wrote df to\", save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
