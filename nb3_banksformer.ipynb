{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swymtxpl7W7w"
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook can train a model to generate sythetic data.   \n",
    "Ensure the 'ds_suffix' matches the one used to generated the dataset (Under \"Set input dataset\" & in create_dataset notebook)  \n",
    "Parameters for generating data (seq_len, number of seqs) are near bottom (Under \"Generate Full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JjJJyJTZYebt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pXzVhU34zWEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cCvXbPkccV1"
   },
   "source": [
    "### Set input dataset and nb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_KEY_ORDER is ['td_sc', 'month', 'day', 'dow', 'tcode_num', 'log_amount_sc']\n",
      "If this is not correct, edit my_lib/field_config.py and re-run notebook\n"
     ]
    }
   ],
   "source": [
    "from my_lib.field_config import *\n",
    "ds_suffix = \"-czech\"\n",
    "nb_id = \"vf1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14354, 81, 24), (14354, 80, 6), (14354,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tensor = np.load(f\"stored_data/inp_tensor-{ds_suffix}.npy\")\n",
    "tar_tensor = np.load(f\"stored_data/tar_tensor-{ds_suffix}.npy\")\n",
    "attributes = np.load(f\"stored_data/attributes-{ds_suffix}.npy\")\n",
    "\n",
    "inp_tensor.shape, tar_tensor.shape, attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqs, n_steps, n_feat_inp = inp_tensor.shape\n",
    "n_feat_tar = tar_tensor.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import load_data_encoder\n",
    "data_encoder = load_data_encoder(ds_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and create tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_cv, inds_tr, inds_cv, targ_tr, targ_cv = train_test_split(\n",
    "    inp_tensor, np.arange(n_seqs), tar_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((81, 24), (80, 6)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tr = tf.data.Dataset.from_tensor_slices((x_tr.astype(np.float32), targ_tr.astype(np.float32)))\n",
    "ds_cv = tf.data.Dataset.from_tensor_slices((x_cv.astype(np.float32), targ_cv.astype(np.float32)))\n",
    "\n",
    "ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BUN_jLBTwNxk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import make_batches\n",
    "\n",
    "BUFFER_SIZE = ds_tr.cardinality().numpy()\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_batches = make_batches(ds_tr, BUFFER_SIZE, BATCH_SIZE)\n",
    "val_batches = make_batches(ds_cv, BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "sample_batch = next(iter(train_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SoX0-vd1hue",
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZOJUSB1T8GjM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "loss_scce_logit = SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "loss_scce_probit = SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "\n",
    "loss_mse = MeanSquaredError(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(tf.reduce_sum(seq, axis=2), 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_parts = []\n",
    "    loss_parts_weighted = []\n",
    "\n",
    "    for k, k_pred in pred.items():\n",
    "\n",
    "        st = FIELD_STARTS_TAR[k]\n",
    "        end = st + FIELD_DIMS_TAR[k]\n",
    "\n",
    "        if k in ONE_HOT_DIMS:\n",
    "            loss_ = loss_scce_logit(real[:, :, st:end], k_pred)\n",
    "        elif k in CLOCK_FIELDS:\n",
    "            loss_ = loss_scce_probit(real[:, :, st:end], clock_to_onehot(k, k_pred))\n",
    "        else:\n",
    "            loss_ = loss_mse(real[:, :, st:end], k_pred)\n",
    "\n",
    "        mask = tf.math.logical_not(tf.math.equal(tf.reduce_sum(real, axis=2), 0))\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask) \n",
    "\n",
    "        loss_parts.append(loss_)\n",
    "        loss_parts_weighted.append(loss_ * LOSS_WEIGHTS[k])\n",
    "\n",
    "    return tf.reduce_sum(loss_parts_weighted), loss_parts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import bulk_encode_time_value\n",
    "\n",
    "EPS_CLOCKP = 0.01\n",
    "\n",
    "CLOCKS = {}\n",
    "for k, val in CLOCK_FIELDS.items():\n",
    "    CLOCKS[k] = tf.constant(bulk_encode_time_value(np.arange(val), val), dtype=tf.float32)\n",
    "\n",
    "def clock_to_probs(pt, pts):\n",
    "    \n",
    "    ds = tf.constant(pts) - pt\n",
    "    sq_ds = np.sum(tf.square(ds+EPS_CLOCKP), axis=1)\n",
    "    raw_ps = 1/ sq_ds   \n",
    "    \n",
    "    return raw_ps / np.sum(raw_ps)\n",
    "\n",
    "\n",
    "\n",
    "def clock_to_onehot(k, vals):\n",
    "    orig_shape = vals.shape\n",
    "\n",
    "    vals = tf.reshape(vals, (-1, orig_shape[-1]))\n",
    "\n",
    "    return np.array([clock_to_probs(p, CLOCKS[k]) for p in vals]).reshape(*orig_shape[:-1], -1)   \n",
    "\n",
    "\n",
    "CLOCK_FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Banksformer configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATIONS = {\n",
    "    \"td_sc\": \"relu\",\n",
    "    \"log_amount_sc\": \"relu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnJn5SLA2ahP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EARLY_STOP = 2\n",
    "EPOCHS = 80\n",
    "\n",
    "\n",
    "num_layers_enc = None\n",
    "opt_name = \"adam\"\n",
    "dff = 128\n",
    "num_layers_dec = 4\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1\n",
    "dr = dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "ONE_HOT_DIMS, FIELD_DIMS, FIELD_STARTS, FIELD_DIMS_TAR, FIELD_STARTS_TAR = get_field_info(ds_suffix)\n",
    "\n",
    "config[\"PRE_DATE_ORDER\"] = PRE_DATE_ORDER\n",
    "config[\"DATE_ORDER\"] = DATE_ORDER\n",
    "config[\"POST_DATE_ORDER\"] = POST_DATE_ORDER\n",
    "config[\"FIELD_STARTS\"] = FIELD_STARTS\n",
    "config[\"FIELD_DIMS\"] = FIELD_DIMS\n",
    "config[\"ACTIVATIONS\"] = ACTIVATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bbvmaKNiznHZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:02\n",
      "Begin running num_layers_dec_4-d_model_128-num_heads_4-i_0-dr_0.1-dff_64-opt_adam-l_loss_mse_lwi-0\n",
      "Epoch 1 Batch 0 Loss 20.7562\n",
      "Epoch 1 Batch 50 Loss 8.6966\n",
      "Epoch 1 Batch 100 Loss 7.1193\n",
      "Epoch 1 Batch 150 Loss 6.3980\n",
      "Epoch 1 Loss 6.0828\n",
      "** on validation data loss is 4.3464\n",
      "Not recording acc: 'Transformer' object has no attribute 'acc_function'\n",
      "Time taken for 1 epoch: 967.06 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/num_layers_dec_4-d_model_128-num_heads_4-i_0-dr_0__1-dff_64-opt_adam-l_loss_mse_lwi-0--czech-vf1/ckpt-1\n",
      "Epoch 2 Batch 0 Loss 4.2712\n",
      "Epoch 2 Batch 50 Loss 4.2055\n",
      "Epoch 2 Batch 100 Loss 4.0893\n",
      "Epoch 2 Batch 150 Loss 3.9701\n",
      "Epoch 2 Loss 3.9025\n",
      "** on validation data loss is 3.5096\n",
      "Time taken for 1 epoch: 943.80 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/num_layers_dec_4-d_model_128-num_heads_4-i_0-dr_0__1-dff_64-opt_adam-l_loss_mse_lwi-0--czech-vf1/ckpt-2\n",
      "Epoch 3 Batch 0 Loss 3.4788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-959807aa5b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m#                         continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEARLY_STOP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/verafin_proj_2/czech-banking-data/lpetrocelli-some-translatedreformatted-czech-banking-data/fresh_pull/banksformer-forvf copy/my_lib/BanksformerGen.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_batches, x_cv, y_cv, epochs, early_stop, print_every, ckpt_every, mid_epoch_updates)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/verafin_proj_2/czech-banking-data/lpetrocelli-some-translatedreformatted-czech-banking-data/fresh_pull/banksformer-forvf copy/my_lib/BanksformerGen.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, inp, tar)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                          dec_padding_mask)\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0dce8f5ce773>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(real, pred)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_scce_logit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCLOCK_FIELDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_scce_probit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclock_to_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;31m#         elif k == \"td\":  # just use mse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#             loss_ = tf.cast(loss_td(real[:, :, st:end], k_pred), tf.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-fe55761743fa>\u001b[0m in \u001b[0;36mclock_to_onehot\u001b[0;34m(k, vals)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclock_to_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0morig_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-fe55761743fa>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclock_to_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0morig_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-fe55761743fa>\u001b[0m in \u001b[0;36mclock_to_probs\u001b[0;34m(pt, pts)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msq_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mEPS_CLOCKP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mraw_ps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0msq_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    466\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    469\u001b[0m         _ctx, \"AddV2\", name, x, y)\n\u001b[1;32m    470\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from my_lib.BanksformerGen import Transformer\n",
    "\n",
    "\n",
    "\n",
    "all_models = []\n",
    "for_df = []\n",
    "\n",
    "\n",
    "def to_num(x):\n",
    "    try: return int(x)\n",
    "    except: return float(x)\n",
    "\n",
    "    \n",
    "def id_str_to_folder(id_str):\n",
    "    return id_str.replace(\".\", \"__\")\n",
    "beta = 1\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS = {'balance': 0.25, # lw2\n",
    " 'td_sc':1.,\n",
    " 'year': 0.5,\n",
    " 'month': 0.15,\n",
    " 'day': 0.25,\n",
    " 'dow': 0.1,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS_0 = {'balance': 0.025, # lw2\n",
    " 'td_sc':1.,\n",
    " 'year': 0.05,\n",
    " 'month': 0.015,\n",
    " 'day': 0.025,\n",
    " 'dow': 0.01,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS_1 = {'balance': 0.025, # lw2\n",
    " 'td_sc':1.,\n",
    " 'year': 0.05,\n",
    " 'month': 0.015,\n",
    " 'day': 0.025,\n",
    " 'dow': 0.01,\n",
    " 'tcode_num': 2.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lws = [LOSS_WEIGHTS_0]\n",
    "\n",
    "td_loss_fns = [(loss_mse, \"loss_mse\")]\n",
    "\n",
    "for i in range(1):\n",
    "    for dff in [64]:\n",
    "        for td_loss_fn, name in td_loss_fns:\n",
    "            for lwi, LOSS_WEIGHTS in enumerate(lws):\n",
    "#             for d_model in [64]:\n",
    "                for num_heads in [4]:\n",
    "                \n",
    "                    loss_td = td_loss_fn\n",
    "\n",
    "                \n",
    "                    print(datetime.datetime.now().strftime(\"%H:%M\"))\n",
    "\n",
    "\n",
    "                    transformer = Transformer(\n",
    "                        num_layers_enc=num_layers_enc, num_layers_dec=num_layers_dec,\n",
    "                        d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        dff=dff,\n",
    "                        maximum_position_encoding=256,\n",
    "                       net_info = FIELD_DIMS.items(), \n",
    "                        inp_dim = n_feat_inp,\n",
    "                        final_dim= max(n_feat_tar, n_feat_inp),\n",
    "                        config=config,\n",
    "                        rate=dr)\n",
    "                    \n",
    "                    optimizer = tf.keras.optimizers.Adam()\n",
    "                    transformer.optimizer =  optimizer\n",
    "                    \n",
    "#                     LOSS_WEIGHTS = lws[lwi]\n",
    "                    transformer.loss_function = loss_function\n",
    "                    transformer.LOSS_WEIGHTS = LOSS_WEIGHTS\n",
    "\n",
    "                    id_str = f\"num_layers_dec_{num_layers_dec}-d_model_{d_model}-num_heads_{num_heads}-i_{i}\\\n",
    "-dr_{dr}-dff_{dff}-opt_{opt_name}-l_{name}_lwi-{lwi}\"\n",
    "                    \n",
    "                    print(\"Begin running\", id_str)\n",
    "                    transformer.id_str = id_str\n",
    "\n",
    "\n",
    "                    all_models.append(transformer)\n",
    "                    transformer.compile()\n",
    "                    \n",
    "                    \n",
    "                    transformer.checkpoint_path = f\"./checkpoints/{id_str_to_folder(transformer.id_str)}-{ds_suffix}-{nb_id}\"\n",
    "                    transformer.ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                                               optimizer=optimizer)\n",
    "                    transformer.ckpt_manager = tf.train.CheckpointManager(transformer.ckpt, \n",
    "                                                                          transformer.checkpoint_path, max_to_keep=EARLY_STOP)\n",
    "                    \n",
    "                    if transformer.ckpt_manager.latest_checkpoint:\n",
    "                        transformer.ckpt.restore(transformer.ckpt_manager.latest_checkpoint)\n",
    "                        print('Latest checkpoint restored!!')    \n",
    "                        continue\n",
    "              \n",
    "                    transformer.fit(train_batches, x_cv, targ_cv, epochs= EPOCHS, early_stop=EARLY_STOP, print_every=50, ckpt_every = 1)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                    for_df.append((num_layers_dec, d_model, num_heads, i, dr, beta, dff , name,\n",
    "                                   np.min(transformer.results[\"val_loss\"]), opt_name, transformer.id_str))\n",
    "                    \n",
    "                    df = pd.DataFrame.from_records(for_df, columns=['num_layers_dec', 'd_model', 'num_heads', 'i', \"dr\", \"beta\",\\\n",
    "                                                                    \"dff\", \"loss name\",\n",
    "                                                                    \"val loss\", \"opt name\",\"id_str\"]).sort_values(\"val loss\")\n",
    "                    \n",
    "                    df.to_csv(f\"generation_results/df{datetime.datetime.now().strftime('%H_%M')}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(for_df, columns=['num_layers_dec', 'd_model', 'num_heads', 'i', \"dr\", \"beta\", \"dff\", \"lwi\",\n",
    "                                                \"val loss\", \"opt name\",\"id_str\"]).sort_values(\"val loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_layers_dec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>i</th>\n",
       "      <th>dr</th>\n",
       "      <th>beta</th>\n",
       "      <th>dff</th>\n",
       "      <th>lwi</th>\n",
       "      <th>val loss</th>\n",
       "      <th>opt name</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [num_layers_dec, d_model, num_heads, i, dr, beta, dff, lwi, val loss, opt name, id_str]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df.sort_values(\"val loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_layers_dec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>i</th>\n",
       "      <th>dr</th>\n",
       "      <th>beta</th>\n",
       "      <th>dff</th>\n",
       "      <th>lwi</th>\n",
       "      <th>val loss</th>\n",
       "      <th>opt name</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [num_layers_dec, d_model, num_heads, i, dr, beta, dff, lwi, val loss, opt name, id_str]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df.sort_values([\"lwi\", \"val loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = all_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate  \n",
    "Warning: Code below is not nice and should be refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_YEARS_SPAN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.layers.core.Dense"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('td_sc', 1), ('month', 2), ('day', 2), ('dow', 2), ('tcode_num', 16), ('log_amount_sc', 1)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_DIMS.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running clocks[7] = np.array([encode_time_value(val, 7) for val in range(7)])\n",
      "Running clocks[31] = np.array([encode_time_value(val, 31) for val in range(31)])\n",
      "Running clocks[12] = np.array([encode_time_value(val, 12) for val in range(12)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 31, 12])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_lib.encoding import encode_time_value\n",
    "#, decode_time_value\n",
    "\n",
    "clocks = {}\n",
    "for max_val in [7, 31, 12]:\n",
    "    cmd = f\"clocks[{max_val}] = np.array([encode_time_value(val, {max_val}) for val in range({max_val})])\"\n",
    "    print(\"Running\", cmd)\n",
    "    exec(cmd)\n",
    "    \n",
    "clocks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import factorial\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "#############  Signatures of pdf/pmfs are (pred, real), this is opposite of losses  #############\n",
    "\n",
    "# def pmf_poisson(l, k):\n",
    "#     return l**k * np.exp(-l) / factorial(k)\n",
    "\n",
    "\n",
    "# def expon_pdf(l, x):\n",
    "#     return l * np.exp(-l*x)\n",
    "\n",
    "\n",
    "def norm_pdf(mean, x):\n",
    "    return norm.pdf(x, loc=mean)\n",
    "    \n",
    "pmf = norm_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = data_encoder.START_DATE \n",
    "\n",
    "if type(START_DATE) == str:\n",
    "    START_DATE = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "    \n",
    "    \n",
    "\n",
    "END_DATE = START_DATE.replace(year = START_DATE.year+ MAX_YEARS_SPAN)\n",
    "\n",
    "ALL_DATES = [START_DATE + datetime.timedelta(i) for i in range((END_DATE - START_DATE).days)]\n",
    "\n",
    "AD = np.array([(d.month % 12, d.day % 31, d.weekday() % 7, i, d.year) for i, d in enumerate(ALL_DATES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'td_sc': 0,\n",
       " 'month': 1,\n",
       " 'day': 3,\n",
       " 'dow': 5,\n",
       " 'tcode_num': 7,\n",
       " 'log_amount_sc': 23}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import create_masks\n",
    "\n",
    "    \n",
    "def reencode_net_prediction(net_name, predictions):\n",
    "    \n",
    "    date_info = {'month':12, 'day':31, 'dow':7}\n",
    "    batch_size = predictions.shape[0]\n",
    "    \n",
    "    if net_name in ['balance', 'td_sc', 'dss', \"log_amount_sc\"]:\n",
    "        return predictions\n",
    "    \n",
    "#     elif net_name == \"year\":\n",
    "#         return tf.round(predictions/YEAR_SCALE)*YEAR_SCALE\n",
    "    \n",
    "    elif net_name in date_info.keys():\n",
    "        return bulk_nearest_clock_enc(predictions, max_val=date_info[net_name])\n",
    "    \n",
    "    elif net_name == \"tcode_num\":\n",
    "        tcode_len = ONE_HOT_DIMS[\"tcode_num\"]\n",
    "        choices = np.arange(tcode_len)\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, data_encoder.n_tcodes)\n",
    "        choosen =  np.reshape([np.random.choice(choices, p=p) for p in ps], newshape=(batch_size, -1))\n",
    "        return tf.one_hot(choosen, depth=tcode_len)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f\"Got invalid net_name: {net_name}\")\n",
    "\n",
    "days_per_month = np.array([(datetime.date(1990, month, 1) - datetime.timedelta(1)).day for month in range(1,13)]) # 0 = dec\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def get_short_name(tcode):\n",
    "    return short_names[tcode]\n",
    "\n",
    "# @np.vectorize\n",
    "# def get_date_str(yyyy, mm, dd):\n",
    "#     return f\"{yyyy}/{mm:02d}/{dd:02d}\"\n",
    "\n",
    "@np.vectorize\n",
    "def get_date_str(mm, dd):\n",
    "    return f\"{mm:02d}/{dd:02d}\"\n",
    "\n",
    "\n",
    "def bulk_decode(seqs, start_dates, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    # *****\n",
    "#     ages = age_scaler.inverse_transform(seqs[:, 0, :])\n",
    "    ages = seqs[:, 0, :] * data_encoder.ATTR_SCALE\n",
    "    seqs = seqs[:, 1:, :]\n",
    "    assert np.sum(np.diff(ages)) == 0, f\"Bad formating, expected all entries same in each row, got {ages}\"\n",
    "\n",
    "    \n",
    "    amts = seqs[:, :, FIELD_STARTS[\"log_amount_sc\"]].numpy() * data_encoder.LOG_AMOUNT_SCALE\n",
    "    amts = 10 ** amts\n",
    "    amts = np.round(amts - 1.0, 2)\n",
    "\n",
    "\n",
    "    days_passed = np.round(seqs[:, :, FIELD_STARTS[\"td_sc\"]] *data_encoder.TD_SCALE ).astype(int)\n",
    "  \n",
    "\n",
    "#     years = np.round(seqs[:, :, FIELD_STARTS[\"year\"]]/ YEAR_SCALE).astype(int) + START_YEAR\n",
    "\n",
    "    months = bulk_nearest_clock_ind(seqs[:, :, FIELD_STARTS[\"month\"]: FIELD_STARTS[\"month\"] +2], 12)\n",
    "    \n",
    "    days = bulk_nearest_clock_ind(seqs[:, :, FIELD_STARTS[\"day\"]: FIELD_STARTS[\"day\"] +2], 31)\n",
    "    days[days==0] = days_per_month[months[days==0]]\n",
    "    months[months==0] = 12 # needs to be done after days (above)\n",
    "    date_fields = get_date_str(months, days)\n",
    "    \n",
    "    dpc = np.cumsum(days_passed, axis=1) \n",
    "    dates = np.array([[start_dates[i] + datetime.timedelta(int(d)) for d in dpc[i]]for i in range(len(start_dates))])\n",
    "    \n",
    "    tcode_inds = np.argmax(seqs[:, :, FIELD_STARTS[\"tcode_num\"]: FIELD_STARTS[\"tcode_num\"] + FIELD_DIMS[\"tcode_num\"]], axis=-1)\n",
    "#     tcodes = get_short_name(tcode_inds)\n",
    "\n",
    "    ages = np.repeat(ages[:, 0:1], amts.shape[1], axis=1).astype(int)\n",
    "    \n",
    "    return_vals = amts, tcode_inds, date_fields, days_passed, ages, dates\n",
    "    return_lbls = \"amount\", \"tcode_nums\", \"date_fields\", \"days_passed\", \"age\", \"date\"\n",
    "\n",
    "#     print(\"Shapes of amts, tcode_inds, dates, days_passed, ages\\n\", \n",
    "#           amts.shape, tcode_inds.shape, dates.shape, days_passed.shape, ages.shape)\n",
    "#     print(\"days_passed\", days_passed, type(days_passed))\n",
    "    \n",
    "    if return_df_list:\n",
    "        return [pd.DataFrame.from_records(zip(*x), columns=return_lbls) for x in zip(*return_vals)]\n",
    "    \n",
    "    if return_single_df:\n",
    "        return pd.DataFrame.from_records([x for x in zip(*[x.reshape(-1) for x in return_vals])], columns=return_lbls)\n",
    "    \n",
    "    return return_vals\n",
    "\n",
    "\n",
    "\n",
    "def nearest_clock_ind(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return np.argmin(d_sq)\n",
    "\n",
    "\n",
    "def nearest_clock_enc(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return clock[np.argmin(d_sq)]\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_ind(encs, max_val):\n",
    "    batch_size = encs.shape[0]\n",
    "    inds =  np.array([nearest_clock_ind(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, 2))])\n",
    "    return inds.reshape((batch_size, -1))\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_enc(encs, max_val):\n",
    "\n",
    "    batch_size = encs.shape[0]\n",
    "    new_encs =  np.array([nearest_clock_enc(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, 2))])\n",
    "    \n",
    "    return new_encs.reshape((batch_size, -1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 24)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(np.array([1,2,3])[:, None, None], repeats=n_feat_inp, axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seqs(length, ages, start_dates, greedy_dates = False, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    if return_single_df and return_df_list:\n",
    "        raise Exception(\"At most one of: 'return_single_df' and 'return_df_list' can be true\")\n",
    "    \n",
    "    date_inds = np.array([(d - START_DATE).days for d in start_dates])\n",
    "    \n",
    "    max_length = length\n",
    "\n",
    "    output = np.repeat(np.array(ages)[:, None, None], repeats=n_feat_inp, axis=2) / data_encoder.ATTR_SCALE\n",
    "    \n",
    "    raw_preds = []\n",
    "    raw_preds.append(output)\n",
    "\n",
    "    date_info = None\n",
    "    \n",
    "    \n",
    "    for i in range(max_length):\n",
    "\n",
    "\n",
    "        combined_mask, dec_padding_mask = create_masks(output)\n",
    "\n",
    "        predictions, attn, raw_ps, date_inds, enc_preds, date_info = call_to_generate(transformer, output, \n",
    "                                                 True, \n",
    "                                                 combined_mask, \n",
    "                                                 dec_padding_mask, date_inds, date_info, greedy_dates =greedy_dates)\n",
    "\n",
    "        \n",
    "        raw_preds.append(raw_ps)\n",
    "\n",
    "        enc_preds = tf.reshape(tf.constant(enc_preds), shape=(-1,1, n_feat_inp))\n",
    "\n",
    "        output = tf.concat([output, enc_preds], axis=1)\n",
    "\n",
    "        \n",
    "    return bulk_decode(output, start_dates, return_single_df, return_df_list), output, raw_preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Forward pass through transformer\n",
    "# \n",
    "# Returns: preds, attn_w, raw_preds, inds\n",
    "# the returned preds have multiple timesteps, but we only \n",
    "# care about the last (it's the only new one)\n",
    "def call_to_generate(transformer, tar, training,\n",
    "           look_ahead_mask, dec_padding_mask, start_inds, prev_date_info=None, greedy_dates = True):\n",
    "    \n",
    "\n",
    "    ### Pass through decoder stack ###\n",
    "    dec_output, attention_weights = transformer.decoder(\n",
    "        tar, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "\n",
    "    final_output = transformer.final_layer(dec_output) \n",
    "\n",
    "    \n",
    "    \n",
    "    ### Predict each field  ###\n",
    "    preds = {}\n",
    "    raw_preds = {}\n",
    "    encoded_preds = []\n",
    "    \n",
    "    \n",
    "    ## Pre date fields \n",
    "    for net_name in transformer.pre_date_order:  \n",
    "        \n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "        pred = reencode_net_prediction(net_name, pred) # keeps time step\n",
    "        preds[net_name] = pred\n",
    "        \n",
    "        \n",
    "        encoded_preds.append(pred[:,-1,:])\n",
    "        final_output = tf.concat([final_output, pred], axis=2)\n",
    "        \n",
    "        \n",
    "    ## Date fields\n",
    "    date_parts = {}\n",
    "    for net_name in transformer.date_fields:  \n",
    "        \n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "    # Combine info from all predicted date fields (day, month, dow, td)\n",
    "    pred_date, inds = raw_dates_to_reencoded(raw_preds, start_inds, greedy_decode =greedy_dates)\n",
    "    preds[\"date\"] = pred_date\n",
    "\n",
    "    \n",
    "    encoded_preds.append(pred_date[:,-1,:])\n",
    "    \n",
    "    \n",
    "    # Note to self -> what does this do?\n",
    "    if not prev_date_info is None:   # For first step may be None, or a starting date\n",
    "        pred_date = tf.concat([prev_date_info, pred_date], axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "    final_output = tf.concat([final_output, pred_date], axis=2)  \n",
    "    \n",
    "          \n",
    "    ## Post date fields\n",
    "    for net_name in transformer.post_date_order:  \n",
    "#         print(net_name)\n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "#         print(pred.shape)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "        pred = reencode_net_prediction(net_name, pred)\n",
    "        preds[net_name] = pred\n",
    "        \n",
    "        encoded_preds.append(pred[:,-1,:])\n",
    "        final_output = tf.concat([final_output, pred], axis=-1)   \n",
    "        \n",
    "    \n",
    "#     print(\"start_inds + inds \\n\", start_inds + inds)\n",
    "#     print(\"\\n\\npred_date\\n\", pred_date)\n",
    "#     print(\"\\n\"*5)\n",
    "        \n",
    "    return preds, attention_weights, raw_preds, start_inds + inds, tf.expand_dims(tf.concat(encoded_preds, axis=1), axis=1), pred_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMF_EPS = 1e-6\n",
    "\n",
    "# Takes raw predictions (info about predicted day, month, dow, and days passed) and start inds \n",
    "# (indicate the current date for each of the seqs) \n",
    "# Computes a number of days passed for each based on inputs (either greedily or with sampling)\n",
    "# returns the new_dates (old_dates + days passed) and their indicies\n",
    "def raw_dates_to_reencoded(raw, start_inds,  max_days = 100, greedy_decode=False):\n",
    "    \n",
    "    all_ps = [clock_to_onehot(k, raw[k][:,-1]) for k in [\"month\", \"day\", \"dow\"]]\n",
    "\n",
    "    timesteps = np.zeros(len(start_inds)).astype(int)\n",
    "\n",
    "    for i, (month_ps, day_ps, dow_ps, l_pred, si) in enumerate(zip(*all_ps, raw[\"td_sc\"][:,-1].numpy(), start_inds)):\n",
    "\n",
    "        ps = month_ps[AD[si:si+max_days,0]]*day_ps[AD[si:si+max_days,1]]*dow_ps[AD[si:si+max_days,2]] * \\\n",
    "                pmf(max(PMF_EPS, l_pred)*data_encoder.TD_SCALE, AD[si:si+max_days,3]-si ) \n",
    "\n",
    "        \n",
    "        if greedy_decode:\n",
    "            timesteps[i] = np.argmax(ps)\n",
    "        else:\n",
    "#             print(\"max_days\", \"len(ps)\" ,max_days, len(ps))\n",
    "            timesteps[i] = np.random.choice(max_days, p=ps/sum(ps))\n",
    "        \n",
    "        \n",
    "    inds = start_inds + timesteps\n",
    "    \n",
    "\n",
    "    return tf.expand_dims(\n",
    "                tf.concat([tf.expand_dims(\n",
    "                           timesteps.astype(np.float32)/ data_encoder.TD_SCALE, axis=1), \n",
    "#                            AD[inds, 4:5]*YEAR_SCALE,\n",
    "                           bulk_encode_time_value(AD[inds, 0], 12),\n",
    "                           bulk_encode_time_value(AD[inds, 1], 31),\n",
    "                           bulk_encode_time_value(AD[inds, 2], 7)\n",
    "              ], axis=1), axis=1), timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    1,    4,    0, 1993],\n",
       "       [   1,    4,    0,    3, 1993]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD[[0,3], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoder.n_tcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_dfs, seqs, raw = generate_seqs(length= 25, \n",
    "                          ages=[75, 25], \n",
    "                          start_dates=[START_DATE, START_DATE+datetime.timedelta(days=1)], \n",
    "                          greedy_dates=False,\n",
    "                          return_df_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>tcode_nums</th>\n",
       "      <th>date_fields</th>\n",
       "      <th>days_passed</th>\n",
       "      <th>age</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.089996</td>\n",
       "      <td>2</td>\n",
       "      <td>01/03</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1205.640015</td>\n",
       "      <td>0</td>\n",
       "      <td>01/05</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2501.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>01/08</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1340.390015</td>\n",
       "      <td>8</td>\n",
       "      <td>01/11</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2334.979980</td>\n",
       "      <td>7</td>\n",
       "      <td>01/15</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>636.659973</td>\n",
       "      <td>2</td>\n",
       "      <td>01/20</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>704.070007</td>\n",
       "      <td>3</td>\n",
       "      <td>01/25</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>152.899994</td>\n",
       "      <td>7</td>\n",
       "      <td>01/30</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>592.859985</td>\n",
       "      <td>0</td>\n",
       "      <td>02/02</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3214.620117</td>\n",
       "      <td>3</td>\n",
       "      <td>02/07</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9149.019531</td>\n",
       "      <td>3</td>\n",
       "      <td>02/13</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6450.509766</td>\n",
       "      <td>3</td>\n",
       "      <td>02/17</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1235.260010</td>\n",
       "      <td>3</td>\n",
       "      <td>02/22</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>319.200012</td>\n",
       "      <td>0</td>\n",
       "      <td>02/26</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>281.399994</td>\n",
       "      <td>0</td>\n",
       "      <td>02/28</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>598.260010</td>\n",
       "      <td>3</td>\n",
       "      <td>03/02</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1143.979980</td>\n",
       "      <td>0</td>\n",
       "      <td>03/05</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1318.640015</td>\n",
       "      <td>5</td>\n",
       "      <td>03/07</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1648.819946</td>\n",
       "      <td>7</td>\n",
       "      <td>03/09</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4250.450195</td>\n",
       "      <td>5</td>\n",
       "      <td>03/14</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3190.429932</td>\n",
       "      <td>3</td>\n",
       "      <td>03/19</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1493.859985</td>\n",
       "      <td>3</td>\n",
       "      <td>03/22</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>315.630005</td>\n",
       "      <td>0</td>\n",
       "      <td>03/27</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>122.290001</td>\n",
       "      <td>2</td>\n",
       "      <td>03/30</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>179.759995</td>\n",
       "      <td>4</td>\n",
       "      <td>04/01</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1993-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         amount  tcode_nums date_fields  days_passed  age        date\n",
       "0    200.089996           2       01/03            1   25  1993-01-03\n",
       "1   1205.640015           0       01/05            2   25  1993-01-05\n",
       "2   2501.750000           0       01/08            3   25  1993-01-08\n",
       "3   1340.390015           8       01/11            3   25  1993-01-11\n",
       "4   2334.979980           7       01/15            4   25  1993-01-15\n",
       "5    636.659973           2       01/20            5   25  1993-01-20\n",
       "6    704.070007           3       01/25            5   25  1993-01-25\n",
       "7    152.899994           7       01/30            5   25  1993-01-30\n",
       "8    592.859985           0       02/02            3   25  1993-02-02\n",
       "9   3214.620117           3       02/07            5   25  1993-02-07\n",
       "10  9149.019531           3       02/13            6   25  1993-02-13\n",
       "11  6450.509766           3       02/17            4   25  1993-02-17\n",
       "12  1235.260010           3       02/22            5   25  1993-02-22\n",
       "13   319.200012           0       02/26            4   25  1993-02-26\n",
       "14   281.399994           0       02/28            2   25  1993-02-28\n",
       "15   598.260010           3       03/02            2   25  1993-03-02\n",
       "16  1143.979980           0       03/05            3   25  1993-03-05\n",
       "17  1318.640015           5       03/07            2   25  1993-03-07\n",
       "18  1648.819946           7       03/09            2   25  1993-03-09\n",
       "19  4250.450195           5       03/14            5   25  1993-03-14\n",
       "20  3190.429932           3       03/19            5   25  1993-03-19\n",
       "21  1493.859985           3       03/22            3   25  1993-03-22\n",
       "22   315.630005           0       03/27            5   25  1993-03-27\n",
       "23   122.290001           2       03/30            3   25  1993-03-30\n",
       "24   179.759995           4       04/01            2   25  1993-04-01"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs_dfs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 80\n",
    "n_seqs_to_generate = len(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14354"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_layers_dec_4-d_model_128-num_heads_4-i_0-dr_0.1-dff_64-opt_adam-l_loss_mse_lwi-0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 29., 29., ..., 46., 46., 46.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin with  num_layers_dec_4-d_model_128-num_heads_4-i_0-dr_0.1-dff_64-opt_adam-l_loss_mse_lwi-0\n",
      "took 455.6913471221924 secs to generate\n",
      "Wrote df to generated_data/gen_num_layers_dec_4-d_model_128-num_heads_4-i_0-dr_0__1-dff_64-opt_adam-l_loss_mse_lwi-0--vf1-len_80.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_seqs_to_generate = 1500\n",
    "\n",
    "\n",
    "start_dates = np.random.choice([START_DATE + datetime.timedelta(i) for i in range(365)], size=n_seqs_to_generate)\n",
    "start_dates\n",
    "\n",
    "seq_ages = np.random.choice(attributes, size=n_seqs_to_generate)\n",
    "seq_ages\n",
    "\n",
    "all_models\n",
    "\n",
    "for i in range(len(all_models)):\n",
    "    \n",
    "    transformer = all_models[i]\n",
    "    \n",
    "    print(\"Begin with \", transformer.id_str)\n",
    "\n",
    "    start = time.time()\n",
    "    full_df, seqs, raw = generate_seqs(length= seq_len, \n",
    "                                       ages=seq_ages, \n",
    "                                       start_dates= start_dates, \n",
    "                                       return_single_df=True )\n",
    "    \n",
    "    full_df[\"account_id\"] = np.arange(len(full_df)) // seq_len\n",
    "    full_df[\"tcode\"] = full_df[\"tcode_nums\"].apply(lambda x: data_encoder.NUM_TO_TCODE[x])\n",
    "\n",
    "    print(f\"took {time.time() - start} secs to generate\")\n",
    "\n",
    "    save_as = f\"generated_data/gen_{id_str_to_folder(transformer.id_str)}--{nb_id}-len_{seq_len}.csv\"\n",
    "\n",
    "\n",
    "    full_df.to_csv(save_as)\n",
    "    print(\"Wrote df to\", save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>tcode_nums</th>\n",
       "      <th>date_fields</th>\n",
       "      <th>days_passed</th>\n",
       "      <th>age</th>\n",
       "      <th>date</th>\n",
       "      <th>account_id</th>\n",
       "      <th>tcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133.389999</td>\n",
       "      <td>2</td>\n",
       "      <td>06/25</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>1993-06-25</td>\n",
       "      <td>0</td>\n",
       "      <td>interest_cr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>635.330017</td>\n",
       "      <td>3</td>\n",
       "      <td>06/29</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>1993-06-29</td>\n",
       "      <td>0</td>\n",
       "      <td>cash_db_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1299.930054</td>\n",
       "      <td>3</td>\n",
       "      <td>07/03</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>1993-07-03</td>\n",
       "      <td>0</td>\n",
       "      <td>cash_db_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1794.300049</td>\n",
       "      <td>0</td>\n",
       "      <td>07/07</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>1993-07-07</td>\n",
       "      <td>0</td>\n",
       "      <td>cash_cr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6470.669922</td>\n",
       "      <td>3</td>\n",
       "      <td>07/11</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>1993-07-11</td>\n",
       "      <td>0</td>\n",
       "      <td>cash_db_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>756.359985</td>\n",
       "      <td>0</td>\n",
       "      <td>09/24</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>1993-09-24</td>\n",
       "      <td>1499</td>\n",
       "      <td>cash_cr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>160.800003</td>\n",
       "      <td>2</td>\n",
       "      <td>09/26</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>1993-09-26</td>\n",
       "      <td>1499</td>\n",
       "      <td>interest_cr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>898.440002</td>\n",
       "      <td>1</td>\n",
       "      <td>09/28</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>1993-09-28</td>\n",
       "      <td>1499</td>\n",
       "      <td>bank_cr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>581.700012</td>\n",
       "      <td>0</td>\n",
       "      <td>10/01</td>\n",
       "      <td>3</td>\n",
       "      <td>71</td>\n",
       "      <td>1993-10-01</td>\n",
       "      <td>1499</td>\n",
       "      <td>cash_cr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>1095.890015</td>\n",
       "      <td>1</td>\n",
       "      <td>10/01</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>1993-10-01</td>\n",
       "      <td>1499</td>\n",
       "      <td>bank_cr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             amount  tcode_nums date_fields  days_passed  age        date  \\\n",
       "0        133.389999           2       06/25            2   38  1993-06-25   \n",
       "1        635.330017           3       06/29            4   38  1993-06-29   \n",
       "2       1299.930054           3       07/03            4   38  1993-07-03   \n",
       "3       1794.300049           0       07/07            4   38  1993-07-07   \n",
       "4       6470.669922           3       07/11            4   38  1993-07-11   \n",
       "...             ...         ...         ...          ...  ...         ...   \n",
       "119995   756.359985           0       09/24            2   71  1993-09-24   \n",
       "119996   160.800003           2       09/26            2   71  1993-09-26   \n",
       "119997   898.440002           1       09/28            2   71  1993-09-28   \n",
       "119998   581.700012           0       10/01            3   71  1993-10-01   \n",
       "119999  1095.890015           1       10/01            0   71  1993-10-01   \n",
       "\n",
       "        account_id        tcode  \n",
       "0                0  interest_cr  \n",
       "1                0  cash_db_nan  \n",
       "2                0  cash_db_nan  \n",
       "3                0      cash_cr  \n",
       "4                0  cash_db_nan  \n",
       "...            ...          ...  \n",
       "119995        1499      cash_cr  \n",
       "119996        1499  interest_cr  \n",
       "119997        1499      bank_cr  \n",
       "119998        1499      cash_cr  \n",
       "119999        1499      bank_cr  \n",
       "\n",
       "[120000 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seqs_to_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(start_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
