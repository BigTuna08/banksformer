{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swymtxpl7W7w"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook can train a model to generate sythetic data.   \n",
    "Ensure the 'ds_suffix' matches the one used to generated the dataset (Under \"Set input dataset\" & in create_dataset notebook)  \n",
    "Parameters for generating data (seq_len, number of seqs) are near bottom (Under \"Generate Full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JjJJyJTZYebt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pXzVhU34zWEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set generate info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 80\n",
    "n_seqs_to_generate = None    # if None do same as # of seqs in dataset with ds_suffix (below)\n",
    "age_distribution = None      # dict mapping age->frequency, if None use same as dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cCvXbPkccV1"
   },
   "source": [
    "### Set input dataset and nb_id (Must match the info in the notebook used to train the Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_KEY_ORDER is ['td_sc', 'month', 'day', 'dow', 'tcode_num', 'log_amount_sc']\n",
      "If this is not correct, edit my_lib/field_config.py and re-run notebook\n"
     ]
    }
   ],
   "source": [
    "from my_lib.field_config import *\n",
    "ds_suffix = \"vfdata\"\n",
    "nb_id = \"vf1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inp_tensor = np.load(f\"stored_data/inp_tensor-{ds_suffix}.npy\")\n",
    "tar_tensor = np.load(f\"stored_data/tar_tensor-{ds_suffix}.npy\")\n",
    "attributes = np.load(f\"stored_data/attributes-{ds_suffix}.npy\")\n",
    "\n",
    "inp_tensor.shape, tar_tensor.shape, attributes.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_seqs, n_steps, n_feat_inp = inp_tensor.shape\n",
    "n_feat_tar = tar_tensor.shape[2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from my_lib.encoding import load_data_encoder\n",
    "data_encoder = load_data_encoder(ds_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any fields not here will have activation=None\n",
    "ACTIVATIONS = {\n",
    "    \"td_sc\": \"relu\",\n",
    "    \"log_amount_sc\": \"relu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SoX0-vd1hue",
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZOJUSB1T8GjM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "loss_scce_logit = SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "loss_scce_probit = SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "\n",
    "loss_mse = MeanSquaredError(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(tf.reduce_sum(seq, axis=2), 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import bulk_encode_time_value\n",
    "\n",
    "EPS_CLOCKP = 0.01\n",
    "\n",
    "CLOCKS = {}\n",
    "for k, val in CLOCK_FIELDS.items():\n",
    "    CLOCKS[k] = tf.constant(bulk_encode_time_value(np.arange(val), val), dtype=tf.float32)\n",
    "\n",
    "def clock_to_probs(pt, pts):\n",
    "    \n",
    "    ds = tf.constant(pts) - pt\n",
    "    sq_ds = np.sum(tf.square(ds+EPS_CLOCKP), axis=1)\n",
    "    raw_ps = 1/ sq_ds   \n",
    "    \n",
    "    return raw_ps / np.sum(raw_ps)\n",
    "\n",
    "\n",
    "\n",
    "def clock_to_onehot(k, vals):\n",
    "    orig_shape = vals.shape\n",
    "\n",
    "    vals = tf.reshape(vals, (-1, orig_shape[-1]))\n",
    "\n",
    "    return np.array([clock_to_probs(p, CLOCKS[k]) for p in vals]).reshape(*orig_shape[:-1], -1)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "num_layers_enc = 2\n",
    "num_layers_dec = 1\n",
    "d_model = 8\n",
    "dff = 32\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "config[\"PRE_DATE_ORDER\"] = PRE_DATE_ORDER\n",
    "config[\"DATE_ORDER\"] = DATE_ORDER\n",
    "config[\"POST_DATE_ORDER\"] = POST_DATE_ORDER\n",
    "config[\"FIELD_STARTS\"] = FIELD_STARTS\n",
    "config[\"FIELD_DIMS\"] = FIELD_DIMS\n",
    "config[\"ACTIVATIONS\"] = ACTIVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat_inp = 69\n",
    "n_feat_tar = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_distribution = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_seqs_to_generate is None:\n",
    "    n_seqs_to_generate = len(np.load(f\"stored_data/attributes-{ds_suffix}.npy\"))\n",
    "    \n",
    "    \n",
    "if age_distribution is None:\n",
    "    attributes = np.load(f\"stored_data/attributes-{ds_suffix}.npy\")\n",
    "    age_distribution = dict([(a, np.sum(attributes==a)/len(attributes)) for a in np.unique(attributes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bbvmaKNiznHZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:09\n",
      "Begin running num_layers_dec_4-d_model_64-num_heads_4-i_0-dr_0.1-dff_64-opt_adam-l_loss_mse_lw2\n",
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "from my_lib.BanksformerGen import Transformer\n",
    "\n",
    "EARLY_STOP = 2\n",
    "EPOCHS = 80\n",
    "\n",
    "opt_name = \"adam\"\n",
    "dr = 0.1\n",
    "                \n",
    "dff = 128\n",
    "num_layers_dec = 4\n",
    "d_model = 128\n",
    "\n",
    "\n",
    "all_models = []\n",
    "for_df = []\n",
    "\n",
    "\n",
    "def to_num(x):\n",
    "    try: return int(x)\n",
    "    except: return float(x)\n",
    "\n",
    "    \n",
    "def id_str_to_folder(id_str):\n",
    "    return id_str.replace(\".\", \"__\")\n",
    "beta = 1\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS = {'balance': 0.25,\n",
    " 'td_sc':1.,\n",
    " 'year': 0.5,\n",
    " 'month': 0.15,\n",
    " 'day': 0.25,\n",
    " 'dow': 0.1,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "lws = [LOSS_WEIGHTS]\n",
    "\n",
    "# td_loss_fns = [(poisson_loss, \"poisson_loss\"), (expon_loss, \"expon_loss\"), (loss_mse, \"loss_mse\")]\n",
    "td_loss_fns = [(loss_mse, \"loss_mse\")]\n",
    "\n",
    "for i in range(1):\n",
    "    for dff in [64]:\n",
    "        for td_loss_fn, name in td_loss_fns:\n",
    "            for d_model in [64]:\n",
    "                for num_heads in [4]:\n",
    "                \n",
    "                    loss_td = td_loss_fn\n",
    "\n",
    "                \n",
    "                    print(datetime.datetime.now().strftime(\"%H:%M\"))\n",
    "\n",
    "\n",
    "                    transformer = Transformer(\n",
    "                        num_layers_enc=num_layers_enc, num_layers_dec=num_layers_dec,\n",
    "                        d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        dff=dff,\n",
    "                        maximum_position_encoding=256,\n",
    "                       net_info = FIELD_DIMS.items(), \n",
    "                        inp_dim = n_feat_inp,\n",
    "                        final_dim= max(n_feat_tar, n_feat_inp),\n",
    "                        config=config,\n",
    "                        rate=dr)\n",
    "                    \n",
    "                    optimizer = tf.keras.optimizers.Adam()\n",
    "                    transformer.optimizer =  optimizer\n",
    "                    \n",
    "                    \n",
    "#                     transformer.loss_function = loss_function\n",
    "#                     transformer.LOSS_WEIGHTS = LOSS_WEIGHTS\n",
    "\n",
    "                    id_str = f\"num_layers_dec_{num_layers_dec}-d_model_{d_model}-num_heads_{num_heads}-i_{i}\\\n",
    "-dr_{dr}-dff_{dff}-opt_{opt_name}-l_{name}_lw2\"\n",
    "                    \n",
    "                    print(\"Begin running\", id_str)\n",
    "                    transformer.id_str = id_str\n",
    "\n",
    "\n",
    "                    all_models.append(transformer)\n",
    "                    transformer.compile()\n",
    "                    \n",
    "                    \n",
    "                    transformer.checkpoint_path = f\"./checkpoints/{id_str_to_folder(transformer.id_str)}--{nb_id}\"\n",
    "                    transformer.ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                                               optimizer=optimizer)\n",
    "                    transformer.ckpt_manager = tf.train.CheckpointManager(transformer.ckpt, \n",
    "                                                                          transformer.checkpoint_path, max_to_keep=EARLY_STOP)\n",
    "                    \n",
    "                    if transformer.ckpt_manager.latest_checkpoint:\n",
    "                        transformer.ckpt.restore(transformer.ckpt_manager.latest_checkpoint)\n",
    "                        print('Latest checkpoint restored!!')    \n",
    "                        continue\n",
    "                    else:\n",
    "                        print(\"Error - could not load\", id_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('generation_results/.ipynb_checkpoints'),\n",
       " PosixPath('generation_results/.DS_Store'),\n",
       " PosixPath('generation_results/df15_58.csv'),\n",
       " PosixPath('generation_results/df18_17.csv'),\n",
       " PosixPath('generation_results/df20_04.csv')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sorted(Path(\"generation_results/\").iterdir(), key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(PosixPath('generation_results/.ipynb_checkpoints'),\n",
       "  'Sun May 16 18:28:18 2021'),\n",
       " (PosixPath('generation_results/.DS_Store'), 'Mon May 31 09:58:44 2021'),\n",
       " (PosixPath('generation_results/df15_58.csv'), 'Fri Jun  4 15:58:09 2021'),\n",
       " (PosixPath('generation_results/df18_17.csv'), 'Fri Jun  4 18:17:45 2021'),\n",
       " (PosixPath('generation_results/df20_04.csv'), 'Fri Jun  4 20:04:51 2021')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x, time.ctime(os.path.getmtime(x))) for x in sorted(Path(\"generation_results/\").iterdir(), key=os.path.getmtime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('generation_results/df20_04.csv')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = sorted(Path(\"generation_results/\").iterdir(), key=os.path.getmtime)[-1]\n",
    "df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>num_layers_dec</th>\n",
       "      <th>d_model</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>i</th>\n",
       "      <th>dr</th>\n",
       "      <th>beta</th>\n",
       "      <th>dff</th>\n",
       "      <th>loss name</th>\n",
       "      <th>val loss</th>\n",
       "      <th>opt name</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>loss_mse</td>\n",
       "      <td>6.342586</td>\n",
       "      <td>adam</td>\n",
       "      <td>num_layers_dec_4-d_model_64-num_heads_4-i_0-dr_0.1-dff_64-opt_adam-l_loss_mse_lw2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  num_layers_dec  d_model  num_heads  i   dr  beta  dff  \\\n",
       "0           0               4       64          4  0  0.1     1   64   \n",
       "\n",
       "  loss name  val loss opt name  \\\n",
       "0  loss_mse  6.342586     adam   \n",
       "\n",
       "                                                                              id_str  \n",
       "0  num_layers_dec_4-d_model_64-num_heads_4-i_0-dr_0.1-dff_64-opt_adam-l_loss_mse_lw2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df = pd.read_csv(df_path) #.sort_values(by=\"val loss\")\n",
    "with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind = result_df[\"val loss\"].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = all_models[best_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_YEARS_SPAN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.layers.core.Dense"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('td_sc', 1), ('month', 2), ('day', 2), ('dow', 2), ('tcode_num', 61), ('log_amount_sc', 1)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_DIMS.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running clocks[7] = np.array([encode_time_value(val, 7) for val in range(7)])\n",
      "Running clocks[31] = np.array([encode_time_value(val, 31) for val in range(31)])\n",
      "Running clocks[12] = np.array([encode_time_value(val, 12) for val in range(12)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 31, 12])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_lib.encoding import encode_time_value\n",
    "#, decode_time_value\n",
    "\n",
    "clocks = {}\n",
    "for max_val in [7, 31, 12]:\n",
    "    cmd = f\"clocks[{max_val}] = np.array([encode_time_value(val, {max_val}) for val in range({max_val})])\"\n",
    "    print(\"Running\", cmd)\n",
    "    exec(cmd)\n",
    "    \n",
    "clocks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import factorial\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "#############  Signatures of pdf/pmfs are (pred, real), this is opposite of losses  #############\n",
    "\n",
    "# def pmf_poisson(l, k):\n",
    "#     return l**k * np.exp(-l) / factorial(k)\n",
    "\n",
    "\n",
    "# def expon_pdf(l, x):\n",
    "#     return l * np.exp(-l*x)\n",
    "\n",
    "\n",
    "def norm_pdf(mean, x):\n",
    "    return norm.pdf(x, loc=mean)\n",
    "    \n",
    "pmf = norm_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = data_encoder.START_DATE \n",
    "\n",
    "if type(START_DATE) == str:\n",
    "    START_DATE = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "    \n",
    "    \n",
    "\n",
    "END_DATE = START_DATE.replace(year = START_DATE.year+ MAX_YEARS_SPAN)\n",
    "\n",
    "ALL_DATES = [START_DATE + datetime.timedelta(i) for i in range((END_DATE - START_DATE).days)]\n",
    "\n",
    "AD = np.array([(d.month % 12, d.day % 31, d.weekday() % 7, i, d.year) for i, d in enumerate(ALL_DATES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'td_sc': 0,\n",
       " 'month': 1,\n",
       " 'day': 3,\n",
       " 'dow': 5,\n",
       " 'tcode_num': 7,\n",
       " 'log_amount_sc': 68}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIELD_STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import create_masks\n",
    "\n",
    "    \n",
    "def reencode_net_prediction(net_name, predictions):\n",
    "    \n",
    "    date_info = {'month':12, 'day':31, 'dow':7}\n",
    "    batch_size = predictions.shape[0]\n",
    "    \n",
    "    if net_name in ['balance', 'td_sc', 'dss', \"log_amount_sc\"]:\n",
    "        return predictions\n",
    "    \n",
    "#     elif net_name == \"year\":\n",
    "#         return tf.round(predictions/YEAR_SCALE)*YEAR_SCALE\n",
    "    \n",
    "    elif net_name in date_info.keys():\n",
    "        return bulk_nearest_clock_enc(predictions, max_val=date_info[net_name])\n",
    "    \n",
    "    elif net_name == \"tcode_num\":\n",
    "        tcode_len = ONE_HOT_DIMS[\"tcode_num\"]\n",
    "        choices = np.arange(tcode_len)\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, data_encoder.n_tcodes)\n",
    "        choosen =  np.reshape([np.random.choice(choices, p=p) for p in ps], newshape=(batch_size, -1))\n",
    "        return tf.one_hot(choosen, depth=tcode_len)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f\"Got invalid net_name: {net_name}\")\n",
    "\n",
    "days_per_month = np.array([(datetime.date(1990, month, 1) - datetime.timedelta(1)).day for month in range(1,13)]) # 0 = dec\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def get_short_name(tcode):\n",
    "    return short_names[tcode]\n",
    "\n",
    "# @np.vectorize\n",
    "# def get_date_str(yyyy, mm, dd):\n",
    "#     return f\"{yyyy}/{mm:02d}/{dd:02d}\"\n",
    "\n",
    "@np.vectorize\n",
    "def get_date_str(mm, dd):\n",
    "    return f\"{mm:02d}/{dd:02d}\"\n",
    "\n",
    "\n",
    "def bulk_decode(seqs, start_dates, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    # *****\n",
    "#     ages = age_scaler.inverse_transform(seqs[:, 0, :])\n",
    "    ages = seqs[:, 0, :] * data_encoder.ATTR_SCALE\n",
    "    seqs = seqs[:, 1:, :]\n",
    "    assert np.sum(np.diff(ages)) == 0, f\"Bad formating, expected all entries same in each row, got {ages}\"\n",
    "\n",
    "\n",
    "    \n",
    "    amts = seqs[:, :, FIELD_STARTS[\"log_amount_sc\"]].numpy() * data_encoder.LOG_AMOUNT_SCALE\n",
    "    amts = 10 ** amts\n",
    "    amts = np.round(amts - 1.0, 2)\n",
    "\n",
    "    n_seqs, n_steps = amts.shape\n",
    "    account_ids = np.repeat(np.arange(n_seqs)[:,None], n_steps, axis=1)\n",
    "\n",
    "    days_passed = np.round(seqs[:, :, FIELD_STARTS[\"td_sc\"]] *data_encoder.TD_SCALE ).astype(int)\n",
    "  \n",
    "\n",
    "#     years = np.round(seqs[:, :, FIELD_STARTS[\"year\"]]/ YEAR_SCALE).astype(int) + START_YEAR\n",
    "\n",
    "    months = bulk_nearest_clock_ind(seqs[:, :, FIELD_STARTS[\"month\"]: FIELD_STARTS[\"month\"] +2], 12)\n",
    "    \n",
    "    days = bulk_nearest_clock_ind(seqs[:, :, FIELD_STARTS[\"day\"]: FIELD_STARTS[\"day\"] +2], 31)\n",
    "    days[days==0] = days_per_month[months[days==0]]\n",
    "    months[months==0] = 12 # needs to be done after days (above)\n",
    "    date_fields = get_date_str(months, days)\n",
    "    \n",
    "    dpc = np.cumsum(days_passed, axis=1) \n",
    "    dates = np.array([[start_dates[i] + datetime.timedelta(int(d)) for d in dpc[i]]for i in range(len(start_dates))])\n",
    "    \n",
    "    tcode_inds = np.argmax(seqs[:, :, FIELD_STARTS[\"tcode_num\"]: FIELD_STARTS[\"tcode_num\"] + FIELD_DIMS[\"tcode_num\"]], axis=-1)\n",
    "#     tcodes = get_short_name(tcode_inds)\n",
    "\n",
    "    ages = np.repeat(ages[:, 0:1], amts.shape[1], axis=1).astype(int)\n",
    "    \n",
    "    return_vals = amts, tcode_inds, date_fields, days_passed, ages, dates, account_ids\n",
    "    return_lbls = \"amount\", \"tcode_nums\", \"date_fields\", \"days_passed\", \"age\", \"date\", \"account_id\"\n",
    "\n",
    "#     print(\"Shapes of amts, tcode_inds, dates, days_passed, ages\\n\", \n",
    "#           amts.shape, tcode_inds.shape, dates.shape, days_passed.shape, ages.shape)\n",
    "#     print(\"days_passed\", days_passed, type(days_passed))\n",
    "    \n",
    "    if return_df_list:\n",
    "        return [pd.DataFrame.from_records(zip(*x), columns=return_lbls) for x in zip(*return_vals)]\n",
    "    \n",
    "    if return_single_df:\n",
    "        return pd.DataFrame.from_records([x for x in zip(*[x.reshape(-1) for x in return_vals])], columns=return_lbls)\n",
    "    \n",
    "    return return_vals\n",
    "\n",
    "\n",
    "\n",
    "def nearest_clock_ind(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return np.argmin(d_sq)\n",
    "\n",
    "\n",
    "def nearest_clock_enc(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return clock[np.argmin(d_sq)]\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_ind(encs, max_val):\n",
    "    batch_size = encs.shape[0]\n",
    "    inds =  np.array([nearest_clock_ind(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, 2))])\n",
    "    return inds.reshape((batch_size, -1))\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_enc(encs, max_val):\n",
    "\n",
    "    batch_size = encs.shape[0]\n",
    "    new_encs =  np.array([nearest_clock_enc(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, 2))])\n",
    "    \n",
    "    return new_encs.reshape((batch_size, -1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seqs(length, ages, start_dates, greedy_dates = False, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    if return_single_df and return_df_list:\n",
    "        raise Exception(\"At most one of: 'return_single_df' and 'return_df_list' can be true\")\n",
    "    \n",
    "    date_inds = np.array([(d - START_DATE).days for d in start_dates])\n",
    "    \n",
    "    max_length = length\n",
    "\n",
    "    output = np.repeat(np.array(ages)[:, None, None], repeats=n_feat_inp, axis=2) / data_encoder.ATTR_SCALE\n",
    "    \n",
    "    raw_preds = []\n",
    "    raw_preds.append(output)\n",
    "\n",
    "    date_info = None\n",
    "    \n",
    "    \n",
    "    for i in range(max_length):\n",
    "\n",
    "\n",
    "        combined_mask, dec_padding_mask = create_masks(output)\n",
    "\n",
    "        predictions, attn, raw_ps, date_inds, enc_preds, date_info = call_to_generate(transformer, output, \n",
    "                                                 True, \n",
    "                                                 combined_mask, \n",
    "                                                 dec_padding_mask, date_inds, date_info, greedy_dates =greedy_dates)\n",
    "\n",
    "        \n",
    "        raw_preds.append(raw_ps)\n",
    "\n",
    "        enc_preds = tf.reshape(tf.constant(enc_preds), shape=(-1,1, n_feat_inp))\n",
    "\n",
    "        output = tf.concat([output, enc_preds], axis=1)\n",
    "\n",
    "        \n",
    "    return bulk_decode(output, start_dates, return_single_df, return_df_list), output, raw_preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Forward pass through transformer\n",
    "# \n",
    "# Returns: preds, attn_w, raw_preds, inds\n",
    "# the returned preds have multiple timesteps, but we only \n",
    "# care about the last (it's the only new one)\n",
    "def call_to_generate(transformer, tar, training,\n",
    "           look_ahead_mask, dec_padding_mask, start_inds, prev_date_info=None, greedy_dates = True):\n",
    "    \n",
    "\n",
    "    ### Pass through decoder stack ###\n",
    "    dec_output, attention_weights = transformer.decoder(\n",
    "        tar, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "\n",
    "    final_output = transformer.final_layer(dec_output) \n",
    "\n",
    "    \n",
    "    \n",
    "    ### Predict each field  ###\n",
    "    preds = {}\n",
    "    raw_preds = {}\n",
    "    encoded_preds = []\n",
    "    \n",
    "    \n",
    "    ## Pre date fields \n",
    "    for net_name in transformer.pre_date_order:  \n",
    "        \n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "        pred = reencode_net_prediction(net_name, pred) # keeps time step\n",
    "        preds[net_name] = pred\n",
    "        \n",
    "        \n",
    "        encoded_preds.append(pred[:,-1,:])\n",
    "        final_output = tf.concat([final_output, pred], axis=2)\n",
    "        \n",
    "        \n",
    "    ## Date fields\n",
    "    date_parts = {}\n",
    "    for net_name in transformer.date_fields:  \n",
    "        \n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "    # Combine info from all predicted date fields (day, month, dow, td)\n",
    "    pred_date, inds = raw_dates_to_reencoded(raw_preds, start_inds, greedy_decode =greedy_dates)\n",
    "    preds[\"date\"] = pred_date\n",
    "\n",
    "    \n",
    "    encoded_preds.append(pred_date[:,-1,:])\n",
    "    \n",
    "    \n",
    "    # Note to self -> what does this do?\n",
    "    if not prev_date_info is None:   # For first step may be None, or a starting date\n",
    "        pred_date = tf.concat([prev_date_info, pred_date], axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "    final_output = tf.concat([final_output, pred_date], axis=2)  \n",
    "    \n",
    "          \n",
    "    ## Post date fields\n",
    "    for net_name in transformer.post_date_order:  \n",
    "#         print(net_name)\n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "#         print(pred.shape)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "        pred = reencode_net_prediction(net_name, pred)\n",
    "        preds[net_name] = pred\n",
    "        \n",
    "        encoded_preds.append(pred[:,-1,:])\n",
    "        final_output = tf.concat([final_output, pred], axis=-1)   \n",
    "        \n",
    "    \n",
    "#     print(\"start_inds + inds \\n\", start_inds + inds)\n",
    "#     print(\"\\n\\npred_date\\n\", pred_date)\n",
    "#     print(\"\\n\"*5)\n",
    "        \n",
    "    return preds, attention_weights, raw_preds, start_inds + inds, tf.expand_dims(tf.concat(encoded_preds, axis=1), axis=1), pred_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMF_EPS = 1e-6\n",
    "\n",
    "# Takes raw predictions (info about predicted day, month, dow, and days passed) and start inds \n",
    "# (indicate the current date for each of the seqs) \n",
    "# Computes a number of days passed for each based on inputs (either greedily or with sampling)\n",
    "# returns the new_dates (old_dates + days passed) and their indicies\n",
    "def raw_dates_to_reencoded(raw, start_inds,  max_days = 100, greedy_decode=False):\n",
    "    \n",
    "    all_ps = [clock_to_onehot(k, raw[k][:,-1]) for k in [\"month\", \"day\", \"dow\"]]\n",
    "\n",
    "    timesteps = np.zeros(len(start_inds)).astype(int)\n",
    "\n",
    "    for i, (month_ps, day_ps, dow_ps, l_pred, si) in enumerate(zip(*all_ps, raw[\"td_sc\"][:,-1].numpy(), start_inds)):\n",
    "\n",
    "        ps = month_ps[AD[si:si+max_days,0]]*day_ps[AD[si:si+max_days,1]]*dow_ps[AD[si:si+max_days,2]] * \\\n",
    "                pmf(max(PMF_EPS, l_pred)*data_encoder.TD_SCALE, AD[si:si+max_days,3]-si ) \n",
    "\n",
    "        \n",
    "        if greedy_decode:\n",
    "            timesteps[i] = np.argmax(ps)\n",
    "        else:\n",
    "#             print(\"max_days\", \"len(ps)\" ,max_days, len(ps))\n",
    "            timesteps[i] = np.random.choice(max_days, p=ps/sum(ps))\n",
    "        \n",
    "        \n",
    "    inds = start_inds + timesteps\n",
    "    \n",
    "\n",
    "    return tf.expand_dims(\n",
    "                tf.concat([tf.expand_dims(\n",
    "                           timesteps.astype(np.float32)/ data_encoder.TD_SCALE, axis=1), \n",
    "#                            AD[inds, 4:5]*YEAR_SCALE,\n",
    "                           bulk_encode_time_value(AD[inds, 0], 12),\n",
    "                           bulk_encode_time_value(AD[inds, 1], 31),\n",
    "                           bulk_encode_time_value(AD[inds, 2], 7)\n",
    "              ], axis=1), axis=1), timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   6,    1,    0,    0, 2020],\n",
       "       [   6,    4,    3,    3, 2020]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AD[[0,3], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoder.n_tcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_dfs, seqs, raw = generate_seqs(length= 25, \n",
    "                          ages=[75, 25], \n",
    "                          start_dates=[START_DATE, START_DATE+datetime.timedelta(days=1)], \n",
    "                          greedy_dates=False,\n",
    "                          return_df_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(np.arange(2)[:,None], 25, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>tcode_nums</th>\n",
       "      <th>date_fields</th>\n",
       "      <th>days_passed</th>\n",
       "      <th>age</th>\n",
       "      <th>date</th>\n",
       "      <th>account_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.450001</td>\n",
       "      <td>0</td>\n",
       "      <td>06/01</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.790001</td>\n",
       "      <td>0</td>\n",
       "      <td>06/01</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.439999</td>\n",
       "      <td>0</td>\n",
       "      <td>06/04</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.919998</td>\n",
       "      <td>0</td>\n",
       "      <td>06/06</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.400002</td>\n",
       "      <td>13</td>\n",
       "      <td>06/08</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34.459999</td>\n",
       "      <td>39</td>\n",
       "      <td>06/10</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>43.610001</td>\n",
       "      <td>0</td>\n",
       "      <td>06/10</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41.220001</td>\n",
       "      <td>0</td>\n",
       "      <td>06/11</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>58.689999</td>\n",
       "      <td>3</td>\n",
       "      <td>06/13</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43.459999</td>\n",
       "      <td>0</td>\n",
       "      <td>06/14</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64.930000</td>\n",
       "      <td>7</td>\n",
       "      <td>06/14</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>51.910000</td>\n",
       "      <td>7</td>\n",
       "      <td>06/14</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>36.849998</td>\n",
       "      <td>17</td>\n",
       "      <td>06/14</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>89.400002</td>\n",
       "      <td>4</td>\n",
       "      <td>06/14</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>47.669998</td>\n",
       "      <td>0</td>\n",
       "      <td>06/17</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>47.200001</td>\n",
       "      <td>0</td>\n",
       "      <td>06/18</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64.570000</td>\n",
       "      <td>3</td>\n",
       "      <td>06/20</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>46.160000</td>\n",
       "      <td>0</td>\n",
       "      <td>06/21</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50.990002</td>\n",
       "      <td>3</td>\n",
       "      <td>06/23</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27.110001</td>\n",
       "      <td>2</td>\n",
       "      <td>06/25</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>38.900002</td>\n",
       "      <td>0</td>\n",
       "      <td>06/27</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>53.090000</td>\n",
       "      <td>22</td>\n",
       "      <td>06/28</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>148.850006</td>\n",
       "      <td>14</td>\n",
       "      <td>06/29</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>65.750000</td>\n",
       "      <td>8</td>\n",
       "      <td>06/30</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>38.570000</td>\n",
       "      <td>26</td>\n",
       "      <td>06/30</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        amount  tcode_nums date_fields  days_passed  age        date  \\\n",
       "0    52.450001           0       06/01            0   75  2020-06-01   \n",
       "1    31.790001           0       06/01            0   75  2020-06-01   \n",
       "2    48.439999           0       06/04            3   75  2020-06-04   \n",
       "3    41.919998           0       06/06            2   75  2020-06-06   \n",
       "4   100.400002          13       06/08            2   75  2020-06-08   \n",
       "5    34.459999          39       06/10            2   75  2020-06-10   \n",
       "6    43.610001           0       06/10            0   75  2020-06-10   \n",
       "7    41.220001           0       06/11            1   75  2020-06-11   \n",
       "8    58.689999           3       06/13            2   75  2020-06-13   \n",
       "9    43.459999           0       06/14            1   75  2020-06-14   \n",
       "10   64.930000           7       06/14            0   75  2020-06-14   \n",
       "11   51.910000           7       06/14            0   75  2020-06-14   \n",
       "12   36.849998          17       06/14            0   75  2020-06-14   \n",
       "13   89.400002           4       06/14            0   75  2020-06-14   \n",
       "14   47.669998           0       06/17            3   75  2020-06-17   \n",
       "15   47.200001           0       06/18            1   75  2020-06-18   \n",
       "16   64.570000           3       06/20            2   75  2020-06-20   \n",
       "17   46.160000           0       06/21            1   75  2020-06-21   \n",
       "18   50.990002           3       06/23            2   75  2020-06-23   \n",
       "19   27.110001           2       06/25            2   75  2020-06-25   \n",
       "20   38.900002           0       06/27            2   75  2020-06-27   \n",
       "21   53.090000          22       06/28            1   75  2020-06-28   \n",
       "22  148.850006          14       06/29            1   75  2020-06-29   \n",
       "23   65.750000           8       06/30            1   75  2020-06-30   \n",
       "24   38.570000          26       06/30            0   75  2020-06-30   \n",
       "\n",
       "    account_id  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            0  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  \n",
       "10           0  \n",
       "11           0  \n",
       "12           0  \n",
       "13           0  \n",
       "14           0  \n",
       "15           0  \n",
       "16           0  \n",
       "17           0  \n",
       "18           0  \n",
       "19           0  \n",
       "20           0  \n",
       "21           0  \n",
       "22           0  \n",
       "23           0  \n",
       "24           0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs_dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_layers_dec_4-d_model_64-num_heads_4-i_0-dr_0.1-dff_64-opt_adam-l_loss_mse_lw2'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.date(2021, 4, 16), datetime.date(2021, 3, 4),\n",
       "       datetime.date(2021, 4, 10), ..., datetime.date(2021, 4, 23),\n",
       "       datetime.date(2021, 4, 20), datetime.date(2021, 4, 26)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_dates = np.random.choice([START_DATE + datetime.timedelta(i) for i in range(365)], size=n_seqs_to_generate)\n",
    "start_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66., 38., 40., ..., 60., 68., 35.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_ages = np.random.choice(attributes, size=n_seqs_to_generate)\n",
    "seq_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1048.3750441074371 secs to generate\n",
      "Wrote df to generated_data/gen_num_layers_dec_4-d_model_64-num_heads_4-i_0-dr_0__1-dff_64-opt_adam-l_loss_mse_lw2--vf1-len_80.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "full_df, seqs, raw = generate_seqs(length= seq_len, \n",
    "                                   ages=seq_ages, \n",
    "                                   start_dates= start_dates, \n",
    "                                   return_single_df=True )\n",
    "\n",
    "print(f\"took {time.time() - start} secs to generate\")\n",
    "\n",
    "save_as = f\"generated_data/gen_{id_str_to_folder(transformer.id_str)}--{nb_id}-len_{seq_len}.csv\"\n",
    "\n",
    "\n",
    "full_df.to_csv(save_as)\n",
    "print(\"Wrote df to\", save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>tcode_nums</th>\n",
       "      <th>date_fields</th>\n",
       "      <th>days_passed</th>\n",
       "      <th>age</th>\n",
       "      <th>date</th>\n",
       "      <th>account_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145.759995</td>\n",
       "      <td>4</td>\n",
       "      <td>04/18</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115.199997</td>\n",
       "      <td>12</td>\n",
       "      <td>04/18</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.959999</td>\n",
       "      <td>12</td>\n",
       "      <td>04/18</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49.650002</td>\n",
       "      <td>0</td>\n",
       "      <td>04/20</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>2021-04-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.180000</td>\n",
       "      <td>5</td>\n",
       "      <td>04/22</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>2021-04-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324315</th>\n",
       "      <td>39.910000</td>\n",
       "      <td>16</td>\n",
       "      <td>07/12</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>4053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324316</th>\n",
       "      <td>111.900002</td>\n",
       "      <td>12</td>\n",
       "      <td>07/12</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>4053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324317</th>\n",
       "      <td>40.860001</td>\n",
       "      <td>0</td>\n",
       "      <td>07/12</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>4053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324318</th>\n",
       "      <td>35.700001</td>\n",
       "      <td>0</td>\n",
       "      <td>07/14</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>2021-07-14</td>\n",
       "      <td>4053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324319</th>\n",
       "      <td>43.689999</td>\n",
       "      <td>0</td>\n",
       "      <td>07/15</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>2021-07-15</td>\n",
       "      <td>4053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324320 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            amount  tcode_nums date_fields  days_passed  age        date  \\\n",
       "0       145.759995           4       04/18            2   66  2021-04-18   \n",
       "1       115.199997          12       04/18            0   66  2021-04-18   \n",
       "2        90.959999          12       04/18            0   66  2021-04-18   \n",
       "3        49.650002           0       04/20            2   66  2021-04-20   \n",
       "4        58.180000           5       04/22            2   66  2021-04-22   \n",
       "...            ...         ...         ...          ...  ...         ...   \n",
       "324315   39.910000          16       07/12            1   35  2021-07-12   \n",
       "324316  111.900002          12       07/12            0   35  2021-07-12   \n",
       "324317   40.860001           0       07/12            0   35  2021-07-12   \n",
       "324318   35.700001           0       07/14            2   35  2021-07-14   \n",
       "324319   43.689999           0       07/15            1   35  2021-07-15   \n",
       "\n",
       "        account_id  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "324315        4053  \n",
       "324316        4053  \n",
       "324317        4053  \n",
       "324318        4053  \n",
       "324319        4053  \n",
       "\n",
       "[324320 rows x 7 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
